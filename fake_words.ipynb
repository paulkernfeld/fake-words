{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview\n",
    "====\n",
    "\"Inatissentis sentere querum\" is a Latin expression meaning, of course, nothing, because those aren't real Latin words. In fact, they are fake Latin words generated by training an algorithm on a Latin text. These words look to me, a non-Latin-knower, like realistic Latin words, and hopefully they do to you, too. Of course, if you know Latin, they probably look terrible, and you're probably already mad at me. Instead, take a look at some of the other languages that I've modeled!\n",
    "\n",
    "This project explores using Markov chains to model word structure in an alphabet-based language, after being trained on a text in that language. This model is then used to generate realistic-looking fake words and to detect \"foreign-looking\" words.\n",
    "\n",
    "What is a \"word?\"\n",
    "----------------\n",
    "There are a lot of ways to define a \"word.\" For this project, I'll define a word to be a sequence of letters and certain approved punctuation marks. As far as approved punctuation, I have included the apostrophe for most alphabets. This means that a contraction like \"don't\" will be counted as \"don't\", rather than being counted as \"dont\" or as two separate words, \"don\" and \"t.\"\n",
    "\n",
    "For languages with accented letters, I've chosen to model accented letters as if each one were its own distinct letter. So, my French \"alphabet\" includes 'E,' 'É,' 'È,' 'Ê,' and 'Ë' as separate \"letters.\" Confusingly, it is possible for a dictionary or text to contain letters that are not in the alphabet! For example, the letters 'J,' 'K,' 'W,' 'X,' and 'Y' are not considered to be part of the Italian alphabet, but they occur frequently in loan words.\n",
    "\n",
    "Below are the alphabets that I've defined for each language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Just cruft. How does anyone survive without these?\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latin: a b c d e f g h i l m n o p q r s t u v w x z\n",
      "german: a ä b c d e f g h i j k l m n o ö p q r s ß t u ü v w x y z\n",
      "spanish: a á b c d e é f g h i í j k l m n ñ o ó p q r s t u ú ü v w x y z\n",
      "french: a à â ä b c ç d e é è ê ë f g h i î ï j k l m n o ö ô p q r s t u û ü ù v w x y z '\n",
      "english: a b c d e f g h i j k l m n o p q r s t u v w x y z '\n",
      "polish: a ą b c ć d e ę f g h i l ł m n ń o ó p r s ś t u x y z ź ż\n",
      "italian: a à b c d e è é f g h i ì l m n o ò p q r s t u ù v z '\n"
     ]
    }
   ],
   "source": [
    "from fake_words.fake_words import LANGUAGES\n",
    "\n",
    "for language in LANGUAGES.values():\n",
    "    print u\"{}: {}\".format(language.name, \" \".join(list(language.alphabet)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first pass: letter frequency\n",
    "================\n",
    "What constitutes a good fake word? \"Mait\" seems like a good fake French word to me, whereas \"xkzz\" does not seem like a good fake French word. How can we differentiate between these words?\n",
    "\n",
    "First up: letter frequency. 'R,' 'A,' 'I,' and 'T' are all commmon letters in French, whereas 'X,' 'K,' and 'Z' are not. This can be used to create a simple language model wherein words with common letters are judged to be likely, and words with uncommon letters are judged to be unlikely.\n",
    "\n",
    "For my first model, I'll define the likelihood of a word to be the product of the likelihood of each letter in the word (a \"[bag-of-letters](https://en.wikipedia.org/wiki/Bag-of-words_model)\" model). In this model, there is one parameter for each letter of the alphabet, which will be the frequency of that letter in the language. I have a text for each language, and I estimate the parameters using maximum likelihood estimates.\n",
    "\n",
    "This model makes it very easy for us to do computations, because all letters are independent of one another! Almost... *too* easy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a short method to print out the most likely words for each language, for each length of word. Since this model calculates word likelihood as a product of letter likelihoods, shorter words will tend to have higher likelihoods. In any language, the most likely word will be the empty string.\n",
    "\n",
    "Since an *M*-letter alphabet can form $M^N$ words of length *N*, exploring the search space of all words is expensive when *N* is large. I have (slightly) optimized this search by pruning the search if the likelihood of the word I'm at is already lower than the likelihood of one of the top words that's already been found. I tried to create a heuristic pruning strategy, but it did not work well.  With my current search, though, I can only generate words of up to four letters in a sane amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_WORD_LENGTH = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from fake_words.fake_words import Language\n",
    "\n",
    "def print_for_max_gram(min_gram, max_gram):\n",
    "    languages = [Language(info, min_gram, max_gram) for info in LANGUAGES.values()]\n",
    "    \n",
    "    for language in languages:\n",
    "        for word_length in range(1, MAX_WORD_LENGTH + 1):        \n",
    "            top_words = language.top_words(word_length, 10)\n",
    "            top_words_formatted = u\", \".join(top_words)\n",
    "            print u\"{}-gram {} length-{}: {}\".format(max_gram, language.info.name, word_length, top_words_formatted)\n",
    "        print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All right, here we go..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram latin length-1: e, i, t, a, u, s, n, o, r, m\n",
      "1-gram latin length-2: ee, ei, ie, ii, et, te, it, ti, ea, ae\n",
      "1-gram latin length-3: eee, eei, iee, eie, iie, eii, iei, iii, ete, eet\n",
      "1-gram latin length-4: eeee, eiee, eeie, ieee, eeei, eiie, eeii, eiei, ieei, iiee\n",
      "\n",
      "1-gram german length-1: e, n, i, r, s, a, t, h, d, l\n",
      "1-gram german length-2: ee, ne, en, ei, ie, er, re, es, se, ae\n",
      "1-gram german length-3: eee, een, ene, nee, iee, eei, eie, eer, ere, ree\n",
      "1-gram german length-4: eeee, neee, eeen, eene, enee, eeie, eeei, eiee, ieee, reee\n",
      "\n",
      "1-gram spanish length-1: e, a, o, s, n, r, i, l, d, t\n",
      "1-gram spanish length-2: ee, ae, ea, aa, eo, oe, ao, oa, se, es\n",
      "1-gram spanish length-3: eee, eea, aee, eae, eaa, aae, aea, aaa, eeo, oee\n",
      "1-gram spanish length-4: eeee, aeee, eeae, eeea, eaee, eaae, eeaa, eaea, aeea, aaee\n",
      "\n",
      "1-gram french length-1: e, a, i, t, s, n, r, u, l, o\n",
      "1-gram french length-2: ee, ea, ae, ei, ie, et, te, se, es, ne\n",
      "1-gram french length-3: eee, aee, eae, eea, eei, eie, iee, eet, ete, tee\n",
      "1-gram french length-4: eeee, eeea, eaee, aeee, eeae, eiee, eeie, ieee, eeei, etee\n",
      "\n",
      "1-gram english length-1: e, t, a, o, n, i, h, s, r, d\n",
      "1-gram english length-2: ee, te, et, ae, ea, eo, oe, ne, en, ei\n",
      "1-gram english length-3: eee, ete, tee, eet, aee, eae, eea, eeo, eoe, oee\n",
      "1-gram english length-4: eeee, teee, eeet, etee, eete, aeee, eeae, eaee, eeea, eeeo\n",
      "\n",
      "1-gram polish length-1: a, i, e, o, z, s, n, r, c, t\n",
      "1-gram polish length-2: aa, ai, ia, ii, ae, ea, ei, ie, ao, oa\n",
      "1-gram polish length-3: aaa, aai, aia, iaa, iia, aii, iai, iii, aae, aea\n",
      "1-gram polish length-4: aaaa, iaaa, aaai, aaia, aiaa, iiaa, iaai, aiia, aaii, iaia\n",
      "\n",
      "1-gram italian length-1: e, i, a, o, n, t, r, l, s, c\n",
      "1-gram italian length-2: ee, ie, ei, ae, ea, eo, oe, ii, ia, ai\n",
      "1-gram italian length-3: eee, eei, iee, eie, eea, eae, aee, oee, eeo, eoe\n",
      "1-gram italian length-4: eeee, ieee, eeie, eeei, eiee, aeee, eaee, eeae, eeea, eoee\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_for_max_gram(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and it's terrible. These do not look like words at all. Nevertheless, there are a few useful things to be gleaned from this.\n",
    "\n",
    "Since a length-1 word is the same as a letter, the length-1 words serve as a double-check on the per-letter parameters. The English letters with highest likelihoods are, in order, \"ETAONI.\" This is close to the generally-accepted [list of most frequent English letters](https://en.wikipedia.org/wiki/Letter_frequency), \"ETAOIN.\" The discrepancy may be because I'm using a slightly old text ([*A Tale of Two Cities*](http://www.gutenberg.org/files/98/98.txt)), or just because the text is too short. Overall, the letter frequency ordering looks good for most languages.\n",
    "\n",
    "Another interesting thing to note here is that letter order does not matter. After 'eee,' for example, 'eet,' 'tee,' and 'ete' are all tied for second place in English.\n",
    "\n",
    "There is a glaring problem with this approach. Although the letter 'E' is very common in English, it is very uncommon to have a word that is composed exclusively of the letter 'E.' To solve this problem, we need a way to model dependencies between letters!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a Markov model\n",
    "===========\n",
    "I have chosen to use a [Markov chain](https://en.wikipedia.org/wiki/Markov_chain) to model the interactions between letters. Instead of modeling each letter individually, now each a word is modeled as a sequence of states, where each state corresponds to a letter. Now there is a parameter for the probability of each transition between any two states. These parameters can still be estimated from the training document.\n",
    "\n",
    "For an *N*-letter alphabet, now we have *N*² state transition parameters to learn. This will create a lot of cases where the parameters aren't covered by the training data. For example, *A Tale of Two Cities* doesn't contain the letter sequences \"XZ\", \"ZX,\" or \"VF,\" amongst many others. In order to prevent this from zeroing out the likelihoods, I've used [add-one smoothing](https://en.wikipedia.org/wiki/Additive_smoothing), adding one to any [bigram](https://en.wikipedia.org/wiki/Bigram) that doesnt appear in the training text. A slightly more principled way to do this would be to add a smoothing parameter *α*, instead of 1, to each count. This parameter *α* could be learned by doing cross-validation on another text. If I weren't too lazy to do it, that is.\n",
    "\n",
    "I'm handling the starts and ends of words specially in order to make the model generate more realistic words. For example, the model should capture that it's extremely uncommon for an English word to end with the letter \"Q.\" To do this, I added Markov chain states for start and end tokens. This idea is taken from computational linguistics, where it is common to put these tokens at the start and end of sentences when modeling sentence structure. These tokens behave more or less like letters, except that they must appear at the start and the end of each word, and they may not appear in the middle of the words. So, the word \"in\" is represented as, ```[\"start token\", \"I\", \"N\", \"end token\"]```.\n",
    "\n",
    "I'm using this Markov model *in addition to* the letter frequency model from the previous section. To calculate the proportional likelihood of a word this way, I multiply the likelihood of the word in the bigram model by its likelihood in the letter frequency model. This results in a function that is not an actual likelihood, but is *proportional* to the likelihood, which works fine for what I'm trying to do. To get the actual likelihood, it would be necessary to divide this proportional likelihood by a very nasty constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-gram latin length-1: e, s, m, t, i, a, o, n, c, d\n",
      "2-gram latin length-2: is, es, am, at, em, te, se, it, ae, ss\n",
      "2-gram latin length-3: ere, ate, tis, qum, ite, ise, tes, tum, ese, int\n",
      "2-gram latin length-4: eres, atis, eris, itis, ates, erem, tere, atum, inte, ites\n",
      "\n",
      "2-gram german length-1: e, s, d, n, t, h, a, m, u, r\n",
      "2-gram german length-2: en, de, er, in, se, ie, un, ge, an, st\n",
      "2-gram german length-3: den, sen, der, ein, ien, gen, ser, ene, ier, eie\n",
      "2-gram german length-4: enen, dein, eien, sten, eren, ende, dene, ener, sein, inen\n",
      "\n",
      "2-gram spanish length-1: e, a, s, l, o, n, y, d, r, i\n",
      "2-gram spanish length-2: de, la, es, do, en, co, as, da, lo, se\n",
      "2-gram spanish length-3: des, ela, era, den, ese, ere, eno, ade, ena, ara\n",
      "2-gram spanish length-4: dela, dera, dese, dere, deno, dena, eses, eres, lade, desa\n",
      "\n",
      "2-gram french length-1: e, s, t, l, a, n, r, i, d, u\n",
      "2-gram french length-2: le, de, ce, es, se, me, re, te, et, it\n",
      "2-gram french length-3: les, des, lle, let, det, ait, ere, ese, ene, ent\n",
      "2-gram french length-4: lere, lese, lene, dere, lent, dese, dene, dent, lele, lait\n",
      "\n",
      "2-gram english length-1: t, s, d, e, o, a, h, n, r, f\n",
      "2-gram english length-2: he, te, an, at, th, as, in, se, ad, is\n",
      "2-gram english length-3: the, hed, and, her, ane, are, ind, hes, hen, ate\n",
      "2-gram english length-4: athe, thed, ther, here, ithe, thes, then, thee, sthe, than\n",
      "\n",
      "2-gram polish length-1: o, i, a, s, m, z, t, n, d, u\n",
      "2-gram polish length-2: ie, ni, na, si, za, ze, po, ta, zy, pa\n",
      "2-gram polish length-3: nie, sie, zie, sza, sze, szy, nia, cie, sta, pie\n",
      "2-gram polish length-4: szie, onie, niem, anie, czie, rzie, dzie, nani, znie, prza\n",
      "\n",
      "2-gram italian length-1: e, a, i, o, s, l, n, t, d, r\n",
      "2-gram italian length-2: de, te, se, co, di, le, pe, to, io, ce\n",
      "2-gram italian length-3: ere, are, ate, ane, pre, ste, ano, ore, ine, ale\n",
      "2-gram italian length-4: dere, tere, sere, core, lere, tore, iore, ante, pere, cone\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_for_max_gram(0, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All right, much better! To me, these are starting to look much more realistic. To kick it up a notch, here is a Markov chain of order two — that is, a model that looks at [trigrams](https://en.wikipedia.org/wiki/Trigram) in addition to bigrams and 1-grams. Bam?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-gram latin length-1: a, e, m, s, c, o, d, t, l, f\n",
      "3-gram latin length-2: es, re, se, et, is, in, te, de, ne, am\n",
      "3-gram latin length-3: que, cum, tum, qui, con, rem, sum, dis, res, dem\n",
      "3-gram latin length-4: quam, quis, atis, quae, etis, orum, inis, atum, esse, quem\n",
      "\n",
      "3-gram german length-1: e, a, o, s, n, f, r, d, u, t\n",
      "3-gram german length-2: er, in, de, en, ge, st, se, un, es, be\n",
      "3-gram german length-3: den, der, gen, ein, sen, und, ben, hen, die, ber\n",
      "3-gram german length-4: sein, inen, eine, sten, unde, wein, lein, dein, icht, sich\n",
      "\n",
      "3-gram spanish length-1: a, y, o, e, s, n, r, u, l, d\n",
      "3-gram spanish length-2: de, la, es, se, no, en, lo, el, do, co\n",
      "3-gram spanish length-3: que, des, las, los, ela, con, cos, del, res, nos\n",
      "3-gram spanish length-4: ente, ques, dela, ento, lara, dera, dese, enta, esta, este\n",
      "\n",
      "3-gram french length-1: à, a, t, e, y, s, m, i, n, r\n",
      "3-gram french length-2: de, le, re, et, ce, se, ne, es, la, me\n",
      "3-gram french length-3: que, les, des, ent, une, res, tre, ces, ait, ses\n",
      "3-gram french length-4: lait, mait, ille, sait, fait, ques, ente, elle, vait, dent\n",
      "\n",
      "3-gram english length-1: a, i, e, o, s, d, f, n, t, '\n",
      "3-gram english length-2: he, re, in, to, th, of, an, as, it, at\n",
      "3-gram english length-3: the, and, her, his, whe, hat, ing, she, ind, was\n",
      "3-gram english length-4: ther, ithe, thed, that, then, this, here, hand, athe, thes\n",
      "\n",
      "3-gram polish length-1: i, a, z, o, s, e, u, c, y, f\n",
      "3-gram polish length-2: na, za, ni, po, do, to, że, ta, mi, ch\n",
      "3-gram polish length-3: nie, sie, mie, się, cie, czy, nia, pie, szy, zie\n",
      "3-gram polish length-4: niem, dzie, przy, prze, anie, onie, znie, niał, prza, trzy\n",
      "\n",
      "3-gram italian length-1: e, a, i, o, è, s, f, n, l, d\n",
      "3-gram italian length-2: di, la, re, no, se, le, ne, to, de, in\n",
      "3-gram italian length-3: che, the, pre, ore, con, pro, non, ine, del, ile\n",
      "3-gram italian length-4: pare, pere, dere, alla, cone, core, dele, none, alle, ante\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_for_max_gram(0, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One strange fake word that I notice here is \"st,\" in fake-German. This doesn't look like a good German word to me, because it does not have any vowels. Since the model never looks at the word as a whole, it has no way of \"counting\" the vowels, consonants, or any other category of letter to ensure that they are occuring in appropriate proportions. Adding in a counting mechanism can make the model a lot more difficult to perform computation on, because it means that every letter has a dependency on every other letter. If you add a vowel at the start of the word, you may need to remove a vowel somewhere else in the word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Generating fake words, finally!\n",
    "=================\n",
    "A functional system to model word probabilities can be used for many different things. To start with, we can find fake words, which are sequences of letters that look realistic, but aren't found in a dictionary.\n",
    "\n",
    "I'll just repeat the previous exercise, but using a dictionary for each language to eliminate sequences of letters that are already real words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_gram = 0\n",
    "max_gram = 3\n",
    "\n",
    "trigram_languages = [Language(info, min_gram, max_gram) for info in LANGUAGES.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-gram latin length-2: am, us, ta, um, ae, le, to, ia, im, pe, di, co\n",
      "3-gram latin length-3: con, dis, tem, int, ium, ine, ant, tes, tis, the, sem, ius\n",
      "3-gram latin length-4: atis, atum, pere, sent, inum, inam, inte, pris, cont, ment, inem, tent\n",
      "\n",
      "3-gram german length-2: de, en, ge, st, se, un, be, le, he, te, me, is\n",
      "3-gram german length-3: hen, ber, men, len, wen, ver, nen, ser, sch, ine, ste, ler\n",
      "3-gram german length-4: inen, sten, unde, lein, icht, sich, eind, dien, iner, dend, eren, aben\n",
      "\n",
      "3-gram spanish length-2: es, co, sa, ma, mo, an, ra, po, ba, pa, da, or\n",
      "3-gram spanish length-3: ela, cos, una, den, ino, men, eno, ena, pas, ado, tra, lla\n",
      "3-gram spanish length-4: ques, dela, ento, lara, dera, esta, endo, cona, elas, quel, quen, ella\n",
      "\n",
      "3-gram french length-2: pe, it, ge, st, is, el, ar, at, al, là, pa, er\n",
      "3-gram french length-3: ent, res, tre, ant, ine, ele, ens, ple, ons, int, che, ous\n",
      "3-gram french length-4: mait, ille, ques, vait, rent, dant, quit, dest, ette, cont, lant, quis\n",
      "\n",
      "3-gram english length-2: th, se, st, le, ve, te, ge, ce, ot, mr, ch, ke\n",
      "3-gram english length-3: whe, ing, ind, hed, ine, che, sed, wer, pre, und, ond, ded\n",
      "3-gram english length-4: ther, ithe, thed, athe, thes, thad, mand, othe, ande, thas, hing, wher\n",
      "\n",
      "3-gram polish length-2: na, za, ni, po, do, to, że, ta, mi, ch, go, le\n",
      "3-gram polish length-3: nie, sie, mie, się, cie, czy, nia, pie, szy, zie, bie, rzy\n",
      "3-gram polish length-4: niem, dzie, przy, prze, anie, onie, znie, niał, prza, trzy, trze, siem\n",
      "\n",
      "3-gram italian length-2: ri, ra, na, an, of, er, th, is, ch, ar, uo, tm\n",
      "3-gram italian length-3: tro, gre, pri, ion, cre, ate, tri, lia, ele, dis, one, mon\n",
      "3-gram italian length-4: dere, cone, dele, tere, inte, anto, dera, gute, illa, tore, into, cona\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from math import log\n",
    "\n",
    "for language in trigram_languages:\n",
    "    for word_length in range(2, MAX_WORD_LENGTH + 1):        \n",
    "        top_words = language.top_words(word_length, 100)\n",
    "        top_nonwords = [w for w in top_words if w not in language.dictionary]\n",
    "        top_nonwords_formatted = u\", \".join(top_nonwords[:12])\n",
    "        print u\"{}-gram {} length-{}: {}\".format(max_gram, language.info.name, word_length, top_nonwords_formatted)\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The less familiar I am with a language, the better the fake words look."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling words progressively\n",
    "===============\n",
    "What's the fun of making fake words if they can only be four letters long? Especially with German? As I mentioned above, the challenge is that the search space of *N*-letter words is too big when *N* > 4. With Latin's measly 23-letter alphabet, there are only six million possible five-letter words. French, though, with 50 total letters, allows 312 million five-letter words! So, instead of trying to cover the entire search space of long words, why not sample?\n",
    "\n",
    "For my first sampler, I sample words progressively from left to right, beginning with the start token and sampling each successive letter conditioned on the letters to its left. It should then be possible to generate *N*-letter words by doing [rejection sampling](https://en.wikipedia.org/wiki/Rejection_sampling), i.e. throwing away all samples that are not of length *N*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latin: a, se, iam, inpe, se, in, orum, culi, manti, dis, in, si, ati, a, ent, to, ner, et, se, a\n",
      "german: ein, ich, al, kan, ha, ein, ein, nen, se, ein, und, es, en, st, en, in, ete, ge, nises, er\n",
      "spanish: to, la, es, es, a, co, e, o, so, se, me, elien, un, a, un, re, a, a, an, no\n",
      "french: en, au, un, a, ens, se, ne, ne, het, ait, il, ce, es, de, pre, de, pet, de, re, en\n",
      "english: st, ho, main, fer, it, in, on, te, unt, an, the, in, an, en, wor, the, she, lan, he, he\n",
      "polish: a, ra, ta, i, s, ta, be, ani, i, o, ucie, z, ch, i, po, a, u, i, czy, sza\n",
      "italian: il, po, te, fo, se, tra, o, e, la, so, i, si, fi, pente, o, pare, se, a, di, a\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIEAAACbCAYAAAD1NIsvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11fc59a50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(trigram_languages))\n",
    "fig.set_figheight(2)\n",
    "fig.set_figwidth(20)\n",
    "\n",
    "for language, axis in zip(trigram_languages, axes):\n",
    "    progressive_samples = language.sample_progressive(1000)\n",
    "    word_lengths, counts = np.unique([len(word) for word in progressive_samples], return_counts=True)\n",
    "    \n",
    "    axis.set_xticks(word_lengths)\n",
    "    axis.set_title(language.info.name)\n",
    "    axis.bar(word_lengths, counts, align='center')\n",
    "    \n",
    "    print u\"{}: {}\".format(language.info.name, u\", \".join(progressive_samples[:20]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, this sampler is spending a lot of time generating short words, and also many repeated words. I suspect that the distribution of word length has an exponential tail. This means that generating long words with rejection sampling will be tragically inefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling words with MCMC\n",
    "=============\n",
    "How can we cover our sample space more effectively? This question always seems to have the same answer: [Markov chain Monte Carlo](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo). Note that the \"Markov chain\" in \"Markov chain Monte Carlo\" is a Markov chain where each state is a *sampled word,* and the chain is a series of samples. This is different than the Markov chain used to model word likelihood, which is a Markov chain where each state is a *letter,* and the chain of letters forms a word.\n",
    "\n",
    "As such, I made a [Metropolis-Hastings sampler](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm). I'm initializing each run with an *N*-letter word comprised of randomly uniformly chosen letters. The proposal distribution makes a 50/50 random choice between the following two options: it can replace a single letter of the previous sample with a randomly uniformly chosen letter, or it can swap two letters. Note that both of these moves keep the word at length *N*.\n",
    "\n",
    "Below, the \"sample\" words represent samples, taken from the sampler at regular intervals (not sequentially, in order to reduce autocorrelation). They are presented in the order that they were sampled. The \"top\" words represent the highest-likelihood words that the sampler finds. They are presented with highest likelihood first. I restart the sampler from random initializations many times, in case it tends to get stuck in certain areas of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-letter latin samples: m, d, a, m, d, a\n",
      "1-letter latin top: a, e, m, s, c, o\n",
      "\n",
      "2-letter latin samples: et, es, ta, is, is, is\n",
      "2-letter latin top: es, re, se, et, is, in\n",
      "\n",
      "3-letter latin samples: ili, sis, lum, con, num, ses\n",
      "3-letter latin top: que, cum, tum, qui, con, rem\n",
      "\n",
      "4-letter latin samples: stat, inti, pram, ciam, quis, gute\n",
      "4-letter latin top: quam, quis, atis, quae, etis, orum\n",
      "\n",
      "5-letter latin samples: intum, sitem, vitum, intis, serum, perae\n",
      "5-letter latin top: conis, quium, estis, intis, perum, intum\n",
      "\n",
      "6-letter latin samples: conium, quisse, iuntem, feriam, silium, ilicat\n",
      "6-letter latin top: querum, sentis, conium, inatis, sentum, queris\n",
      "\n",
      "7-letter latin samples: antatis, nonitis, sentant, inturti, liceres, vitione\n",
      "7-letter latin top: interum, estatis, interis, peratis, sentere, sentiam\n",
      "\n",
      "8-letter latin samples: inateror, restinis, prantent, antatina, quaectus, ratiatis\n",
      "8-letter latin top: quitatis, essentis, senterum, essentum, intentis, senteris\n",
      "\n",
      "9-letter latin samples: ventaessi, mantation, iderantis, diusterum, coraderis, pratquent\n",
      "9-letter latin top: sententis, sententum, interatis, ressentis, peritatis, contentis\n",
      "\n",
      "10-letter latin samples: intarectem, gutentinem, quemantere, quidedisse, catestente, everessius\n",
      "10-letter latin top: intenterum, essenteris, senteratis, intenteris, atissentis, atissentum\n",
      "\n",
      "11-letter latin samples: queraenerae, nontameatis, nespetilium, reissentest, deremeressi, pratquistis\n",
      "11-letter latin top: essententis, sententerum, essententum, conissentis, sententeris, quissentere\n",
      "\n",
      "12-letter latin samples: quosseratiam, oritustentes, consularioss, enterentente, intissimorum, itisilinarei\n",
      "12-letter latin top: quissenteris, sentententis, ressententis, essententere, inatissentis, mentententis\n",
      "\n",
      "\n",
      "1-letter german samples: e, e, a, e, e, s\n",
      "1-letter german top: e, a, o, s, n, f\n",
      "\n",
      "2-letter german samples: se, un, in, te, de, es\n",
      "2-letter german top: er, in, de, en, ge, st\n",
      "\n",
      "3-letter german samples: der, den, sch, mer, den, und\n",
      "3-letter german top: den, der, gen, ein, sen, und\n",
      "\n",
      "4-letter german samples: icht, icht, inen, wein, sein, wein\n",
      "4-letter german top: sein, inen, eine, sten, unde, wein\n",
      "\n",
      "5-letter german samples: einen, einen, vinen, esern, under, augen\n",
      "5-letter german top: unden, einen, under, ichen, schen, einer\n",
      "\n",
      "6-letter german samples: seinen, leines, seinen, denden, dichte, einder\n",
      "6-letter german top: seinen, einden, denden, sichen, nichen, einder\n",
      "\n",
      "7-letter german samples: underer, underte, esichte, seichen, dernein, ineinen\n",
      "7-letter german top: seinden, undenen, sichten, seinder, einenen, nichten\n",
      "\n",
      "8-letter german samples: schenden, einender, undenden, bendende, denender, einesein\n",
      "8-letter german top: undenden, einenden, undender, icheinen, einender, undeinen\n",
      "\n",
      "9-letter german samples: bertheine, dereinder, mandeinen, ereinende, leinderte, reineinde\n",
      "9-letter german top: seinenden, sicheinen, nicheinen, seinender, eindenden, dendenden\n",
      "\n",
      "10-letter german samples: mandesente, ichangesen, warthender, wieltenden, eichendein, wieseinden\n",
      "10-letter german top: undeseinen, eineseinen, undereinen, seindenden, sicheinden, undenenden\n",
      "\n",
      "11-letter german samples: eineinender, denicheinen, heinkeichen, gendereiche, eindenichen, dereseinsch\n",
      "11-letter german top: undendenden, einendenden, seineseinen, undendender, undeseinden, einendender\n",
      "\n",
      "12-letter german samples: weinenbeinen, landendender, seingerntend, allichtenden, vondenenenen, eineseinerte\n",
      "12-letter german top: seinendenden, sicheinenden, nicheinenden, seinendender, eindendenden, dendendenden\n",
      "\n",
      "\n",
      "1-letter spanish samples: a, a, y, a, a, a\n",
      "1-letter spanish top: a, y, o, e, s, n\n",
      "\n",
      "2-letter spanish samples: de, el, as, el, de, vo\n",
      "2-letter spanish top: de, la, es, se, no, en\n",
      "\n",
      "3-letter spanish samples: dea, cas, den, lle, los, ela\n",
      "3-letter spanish top: que, des, las, los, ela, con\n",
      "\n",
      "4-letter spanish samples: cate, arme, ento, mere, ella, haba\n",
      "4-letter spanish top: ente, ques, dela, ento, lara, dera\n",
      "\n",
      "5-letter spanish samples: desca, entas, mates, ellos, derse, menta\n",
      "5-letter spanish top: quera, dente, quela, desta, deste, entes\n",
      "\n",
      "6-letter spanish samples: queron, adorta, restes, unatra, lamara, incoro\n",
      "6-letter spanish top: entera, quente, questa, queste, destra, queras\n",
      "\n",
      "7-letter spanish samples: laseros, derente, sencios, dosande, enadama, lentera\n",
      "7-letter spanish top: entente, entento, ententa, dentera, estente, destera\n",
      "\n",
      "8-letter spanish samples: sencente, sientina, tenteras, enimieno, dentrado, lanosala\n",
      "8-letter spanish top: dentente, destente, ententes, ententen, contente, ententos\n",
      "\n",
      "9-letter spanish samples: merancila, untetente, meronesta, amulleste, saleseras, cestinado\n",
      "9-letter spanish top: ententera, quentente, questente, dententes, ententero, destentes\n",
      "\n",
      "10-letter spanish samples: peranadono, esterentos, mantentina, esasitando, cadanteros, lestidados\n",
      "10-letter spanish top: ententente, ententento, entententa, dententera, estentente, destentera\n",
      "\n",
      "11-letter spanish samples: destentrado, lestestinos, ententerado, despentesto, antantendon, donderestos\n",
      "11-letter spanish top: dententente, destentente, contentente, entententos, sententente, mententente\n",
      "\n",
      "12-letter spanish samples: porestentana, dentarantodo, habandonsido, ellasariande, decestentien, laraterecido\n",
      "12-letter spanish top: entententera, quententente, entententero, ententerante, destientente, estententera\n",
      "\n",
      "\n",
      "1-letter french samples: a, à, à, a, à, à\n",
      "1-letter french top: à, a, t, e, y, s\n",
      "\n",
      "2-letter french samples: et, le, de, es, le, de\n",
      "2-letter french top: de, le, re, et, ce, se\n",
      "\n",
      "3-letter french samples: ent, des, tre, ent, ses, mes\n",
      "3-letter french top: que, les, des, ent, une, res\n",
      "\n",
      "4-letter french samples: inte, mone, ande, mait, sait, cous\n",
      "4-letter french top: lait, mait, ille, sait, fait, ques\n",
      "\n",
      "5-letter french samples: rente, avait, l'ait, daint, deure, leste\n",
      "5-letter french top: était, avait, illes, delle, entre, elles\n",
      "\n",
      "6-letter french samples: ontent, entite, cerais, treure, entait, entres\n",
      "6-letter french top: quelle, entait, delles, entent, celles, dement\n",
      "\n",
      "7-letter french samples: quisent, lantais, ramiles, entrait, dellait, maitait\n",
      "7-letter french top: lestait, destait, comment, laitait, maitait, laitent\n",
      "\n",
      "8-letter french samples: entrison, quitrait, soursent, questait, faitants, laitenda\n",
      "8-letter french top: étaitait, avaitait, questait, étaitent, avaitent, commente\n",
      "\n",
      "9-letter french samples: dellestit, entiniste, antenteme, sontiente, voutaisse, entraiste\n",
      "9-letter french top: illestait, ententait, quelleste, dementait, entaitait, ententent\n",
      "\n",
      "10-letter french samples: ententrout, ensisserit, avaitenait, noutellest, laissaient, laitensier\n",
      "10-letter french top: commentait, lestaitait, destaitait, laitentait, maitentait, cellestait\n",
      "\n",
      "11-letter french samples: étaititrent, pouravaires, avainainent, sensiontant, l'étatrenne, estabrentre\n",
      "11-letter french top: étaitentait, avaitentait, étaitentent, avaitentent, questaitait, illestaient\n",
      "\n",
      "12-letter french samples: cheurairaine, ententelende, seraissellet, luillessesse, puneleindent, lencementeur\n",
      "12-letter french top: ententaitait, étaitentrent, lestaientent, étaitentrait, maitententre, destaientent\n",
      "\n",
      "\n",
      "1-letter english samples: i, i, i, a, i, i\n",
      "1-letter english top: a, i, e, o, s, d\n",
      "\n",
      "2-letter english samples: th, he, be, he, he, be\n",
      "2-letter english top: he, re, in, to, th, of\n",
      "\n",
      "3-letter english samples: and, and, ton, are, the, din\n",
      "3-letter english top: the, and, her, his, whe, hat\n",
      "\n",
      "4-letter english samples: ther, here, wand, here, here, ther\n",
      "4-letter english top: ther, ithe, thed, that, then, this\n",
      "\n",
      "5-letter english samples: tones, hathe, aness, ither, bethe, hathe\n",
      "5-letter english top: there, withe, thand, hathe, nothe, ither\n",
      "\n",
      "6-letter english samples: thison, hathis, mither, withen, ithend, thered\n",
      "6-letter english top: thathe, thered, theres, ithere, theand, wither\n",
      "\n",
      "7-letter english samples: hearled, athaven, dathere, thereat, thather, nothene\n",
      "7-letter english top: theathe, therthe, thather, withere, therand, thereat\n",
      "\n",
      "8-letter english samples: thathere, thistere, hereened, theasser, deathand, thereche\n",
      "8-letter english top: therethe, thathere, therithe, thereand, thereare, therathe\n",
      "\n",
      "9-letter english samples: anotheres, teressame, therisond, thimanded, itheather, thereated\n",
      "9-letter english top: thereathe, thathathe, thathered, theathere, therether, therthere\n",
      "\n",
      "10-letter english samples: theremande, withereare, therwither, tereverand, asithatere, fromeresen\n",
      "10-letter english top: therethere, thereather, therithere, ithereathe, theathathe, theathered\n",
      "\n",
      "11-letter english samples: therthatere, lowereathed, torithathen, thereathist, wingeathand, theretioner\n",
      "11-letter english top: thereathere, therethered, thereathand, therithered, therearithe, therearther\n",
      "\n",
      "12-letter english samples: thathatheart, haillettered, withatheried, thatherester, asitherintat, beeathereand\n",
      "12-letter english top: thathereathe, theretheathe, theathathere, theretherthe, therethather, thatherether\n",
      "\n",
      "\n",
      "1-letter polish samples: s, i, i, o, i, i\n",
      "1-letter polish top: i, a, z, o, s, e\n",
      "\n",
      "2-letter polish samples: mi, ni, na, na, ci, te\n",
      "2-letter polish top: na, za, ni, po, do, to\n",
      "\n",
      "3-letter polish samples: pie, sza, nie, tam, nie, dze\n",
      "3-letter polish top: nie, sie, mie, się, cie, czy\n",
      "\n",
      "4-letter polish samples: abem, anie, seni, przy, anie, śmia\n",
      "4-letter polish top: niem, dzie, przy, prze, anie, onie\n",
      "\n",
      "5-letter polish samples: radzy, obies, nieni, tanie, przem, panie\n",
      "5-letter polish top: panie, odzie, zanie, tanie, niego, tenie\n",
      "\n",
      "6-letter polish samples: pronie, nierzy, przase, niorze, nienie, podzie\n",
      "6-letter polish top: nienie, podzie, niecie, niemie, nierzy, nierze\n",
      "\n",
      "7-letter polish samples: miniele, czeniem, anienie, nialich, przecie, przysto\n",
      "7-letter polish top: niedzie, przenie, nieniem, dzienie, anienie, niednie\n",
      "\n",
      "8-letter polish samples: teraniem, nierzyna, tronieli, zniedzie, czystrza, szystrze\n",
      "8-letter polish top: panienie, nieranie, niedziem, przedzie, dziedzie, strzenie\n",
      "\n",
      "9-letter polish samples: siędziący, ranielach, urzystrzy, miterzeny, czyszamie, prostacie\n",
      "9-letter polish top: nienienie, podzienie, nierzenie, niecienie, nieniecie, nieniemie\n",
      "\n",
      "10-letter polish samples: nieliłonie, tebieszcza, obieniecza, nienienier, poczyłonie, nientelich\n",
      "10-letter polish top: niedzienie, nieniedzie, przenienie, podziedzie, nienieniem, dzienienie\n",
      "\n",
      "11-letter polish samples: nieniecznie, przenałanie, przeczniery, poczniedzie, dniczystrza, przemiliebo\n",
      "11-letter polish top: panienienie, nieranienie, nienieranie, nieniedziem, niedzieniem, zanienienie\n",
      "\n",
      "12-letter polish samples: ciemuterznie, nierzeniesty, czemieniecza, postrzystrzy, zeciecierzna, alieniedziem\n",
      "12-letter polish top: podzienienie, nienierzenie, nierzenienie, nienieniemie, nieniemienie, panieniedzie\n",
      "\n",
      "\n",
      "1-letter italian samples: a, e, a, o, a, a\n",
      "1-letter italian top: e, a, i, o, è, s\n",
      "\n",
      "2-letter italian samples: de, la, no, se, te, de\n",
      "2-letter italian top: di, la, re, no, se, le\n",
      "\n",
      "3-letter italian samples: are, one, ile, ste, ere, ali\n",
      "3-letter italian top: che, the, pre, ore, con, pro\n",
      "\n",
      "4-letter italian samples: inna, mene, fano, gino, ante, cedi\n",
      "4-letter italian top: pare, pere, dere, alla, cone, core\n",
      "\n",
      "5-letter italian samples: costa, secor, antio, anato, senti, dello\n",
      "5-letter italian top: della, delle, nella, nelle, dello, chere\n",
      "\n",
      "6-letter italian samples: itente, intero, anonte, pomend, indene, sergre\n",
      "6-letter italian top: quella, quelle, inella, inelle, antere, conone\n",
      "\n",
      "7-letter italian samples: nonelli, manothe, anation, tandona, nondato, sterand\n",
      "7-letter italian top: conella, nonella, andella, dellare, conelle, nonelle\n",
      "\n",
      "8-letter italian samples: diniente, nonterre, contiche, atitensi, nonatane, datorita\n",
      "8-letter italian top: contente, sentente, nontente, anonella, tentente, contento\n",
      "\n",
      "9-letter italian samples: uteratare, prestento, stantella, nersitate, isseratio, trassenti\n",
      "9-letter italian top: antentere, comentere, dellatere, cononelle, intentere, antenella\n",
      "\n",
      "10-letter italian samples: cerandessa, nosserassi, tattenseri, umatareste, gutendinti, contiontel\n",
      "10-letter italian top: antentente, contentere, comentente, sententere, nontentere, dellatente\n",
      "\n",
      "11-letter italian samples: prenbermare, quellatesse, daineration, conestrithe, berenberena, andeipronon\n",
      "11-letter italian top: sententente, contentento, dellandella, sententento, mententente, pententente\n",
      "\n",
      "12-letter italian samples: orontatrisse, gentinitione, ansenbenelle, perestartene, altriondelli, delitenitato\n",
      "12-letter italian top: antentenella, conontentere, contententio, anontentente, intentenella, nonontentere\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "for language in trigram_languages:\n",
    "    for word_length in xrange(1, 13):\n",
    "        samples, top = language.sample_mh(word_length, max_to_store=6, n_runs=100, n_samples=1000*word_length)\n",
    "\n",
    "        samples_joined = u\", \".join(word for word, prob in samples[:6])\n",
    "        print u\"{}-letter {} samples: {}\".format(word_length, language.info.name, samples_joined)\n",
    "        \n",
    "        top_joined = \", \".join(word for prob, word in reversed(sorted(top, key=itemgetter(0))))\n",
    "        print u\"{}-letter {} top: {}\\n\".format(word_length, language.info.name, top_joined)\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encouragingly, the top-likelihood sampled words match the exhaustively-searched words well! Also encouragingly, the \"top\" words found by the sampler tend to be consistent between runs. These clues indicate that the sampler is probably not completely broken.\n",
    "\n",
    "These words look pretty good. Once they get long, though, certain three-letter sequences of letters start to repeat (e.g. \"dendendenden\" in fake-German). This makes a lot of sense, since the model is only aware of the three-letter neighborhood next to each letter.\n",
    "\n",
    "Using a sampling algorithm to search for high-likelihood points is a slight abuse of the sampler, since that's not what a sampler does. For some reason, it seems to work fine here, probably because the areas it's exploring aren't very large. To get even longer words, it would probably be best to use a real search algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Detecting foreign-looking words\n",
    "=================\n",
    "Time for some more fun! For this tangent, I'll go through the dictionary of each language to find words that seem to belong to other languages. Some of these are obviously loan words, but other ones (e.g. for Latin) just look foreign.\n",
    "\n",
    "In order to get a ranking, I've simply taken the ratio of the likelihood in the destination languages to the likelihood in the source language. Note that this causes longer words to be chosen, which is ranking the words by a kind of *statistical significance*. A more Bayesian approach would entail dividing this ratio by word length, which would more closely measure *effect size*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latin words that look german: andrachnen, andrachlen, dendrachaten, pyrrhichen, jungebar\n",
      "latin words that look spanish: decharmidabar, decharmidabas, apolactizabar, decharmidandos, decharmidando\n",
      "latin words that look french: branchai, transfigurans, parochai, chondrillai, ravennai\n",
      "latin words that look english: asty, branchad, spathad, chelyin, gry\n",
      "latin words that look polish: chrysoprasie, onycha, sardonycha, zelotypo, chrysopastom\n",
      "latin words that look italian: leopardale, leopardali, buttutti, chalaziai, leopardi\n",
      "german words that look latin: computersimulation, quantitativ, observatorium, antiquariat, imperialismus\n",
      "german words that look spanish: violoncello, colorado, parlamentarismus, colonia, casanova\n",
      "german words that look french: quantitativ, montreux, europaparlament, renaissance, portemonnaie\n",
      "german words that look english: showdown, journalismus, oppositionspolitiker, sympathisant, shopping\n",
      "german words that look polish: psychosozial, psychologie, colonia, urania, paparazzi\n",
      "german words that look italian: violoncello, quantitativ, bruttosozialprodukt, piccolo, privatinitiative\n",
      "spanish words that look latin: nequáquam, cumquibus, súmmum, venimécum, tuáutem\n",
      "spanish words that look german: fahrenheit, kirsch, kitsch, angström, sandwich\n",
      "spanish words that look french: tournée, soufflé, forfait, affaire, moisés\n",
      "spanish words that look english: overbooking, ranking, washingtoniano, footing, windsurfing\n",
      "spanish words that look polish: eczema, zinnia, alcuzcuzu, nazi, kamikaze\n",
      "spanish words that look italian: neonazi, chipichipi, nequáquam, katiuska, chischil\n",
      "french words that look latin: praesidium, praesidiums, aegopodium, caesium, cuproaluminium\n",
      "french words that look german: knickerbockers, breitschwanz, zwieback, weltanschauung, weltanschauungs\n",
      "french words that look spanish: zapateado, pronunciamiento, aficionado, smorzando, zapateados\n",
      "french words that look english: knickerbockers, sweepstake, shocking, skinhead, shopping\n",
      "french words that look polish: biodynamie, dyschromatopsie, zymotechnie, szlachta, teledynamie\n",
      "french words that look italian: aggiornamento, prosciutto, taleggio, grazioso, benzodiazepine\n",
      "english words that look latin: praesidium, gynaecium, pseudopodium, nonequilibrium, anaerobium\n",
      "english words that look german: weltschmerz, zeitgeber, braunschweiger, weltschmerzes, lebensraum\n",
      "english words that look spanish: haciendados, osteosarcomata, hacendados, abracadabra, piasaba\n",
      "english words that look french: etiquettes, jeux, jambeaux, oeuvres, questionnaires\n",
      "english words that look polish: psychodynamic, psychodrama, psychobiologic, hypoglycemia, psychodynamics\n",
      "english words that look italian: razzamatazz, razzamatazzes, osteosarcomata, pharmacopoeia, prosopopoeia\n",
      "polish words that look latin: videokonferencji, radiointerferometrii, radiointerferometria, informatorium, quasimodowi\n",
      "polish words that look german: wielkokomórkowe, wielkoskalowe, nawiewiewie, reinwestowane, kinderbalik\n",
      "polish words that look spanish: radiointerferometria, inteligenciak, ferrofosfor, adrenoreceptorów, banderillera\n",
      "polish words that look french: neurotransplantations, savoir, neurotransplantacje, neutrofin, quincunx\n",
      "polish words that look english: walker, detergenowe, endowaskularnej, banderillerowi, wincentowie\n",
      "polish words that look italian: videokonferencji, ultrarentgenowski, immunofluorescencji, radiointerferometria, radiointerferometrii\n",
      "italian words that look latin: cumquibus, protoquamquam, omnium, quamquam, omnibus\n",
      "italian words that look german: kinderheim, wehrmacht, fahrenheit, breitschwanz, zeitgeist\n",
      "italian words that look spanish: calvados, blablabla, endecasillaba, abracadabra, descamisado\n",
      "italian words that look french: nouveau, connaisseur, tourniquet, jeans, voyeur\n",
      "italian words that look english: whiskey, sherry, mystery, hickory, workshop\n",
      "italian words that look polish: mystery, niego, milady, pony, body\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from fake_words.fake_words import try_to_store\n",
    "\n",
    "N_TO_STORE = 5\n",
    "\n",
    "# Loop over every pair of languages\n",
    "for language_source in trigram_languages:\n",
    "    for language_dest in trigram_languages:\n",
    "        # A heap to store our best results\n",
    "        best_words = []\n",
    "\n",
    "        if language_dest is language_source:\n",
    "            continue\n",
    "    \n",
    "        for word in language_source.dictionary:\n",
    "            word_with_tokens = u\" {} \".format(word)\n",
    "            # Subtraction produces a ratio because these are log probabilities\n",
    "            ratio = (language_dest.get_word_log_likelihood(word_with_tokens) - \n",
    "                     language_source.get_word_log_likelihood(word_with_tokens))\n",
    "            try_to_store(best_words, word, ratio, N_TO_STORE)\n",
    "\n",
    "        best_words_pretty = \", \".join(word for prob, word in reversed(sorted(best_words, key=itemgetter(0))))\n",
    "        print u\"{} words that look {}: {}\".format(language_source.info.name, language_dest.info.name, best_words_pretty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All right, now I'm having fun!\n",
    "\n",
    "I always feel slightly offended when I see the words that look English. \"Whiskey.\" \"Showdown.\" \"Branchad.\" \"Spathad.\" \"Overbooking!\" Is this what English looks like to non-English-speakers? To make myself feel better, I imagine a heavy-set German man with a disproportionately large mustache saying, \"braunschweiger,\" \"trochleis,\" and \"weltschmerzes\" very seriously.\n",
    "\n",
    "I also enjoy that the algorithm classifies \"jeans\" as French. Which is true, just ask any pair of Jeans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Conclusion\n",
    "=====\n",
    "Hopefully this article has illustrated the strengths and limitations of using a Markov model for word structure. It seems to work nicely for short words, but for long words it might need to be augmented by 4-grams or 5-grams, or it might need to be replaced by a non-Markov model, such as a tree-based one.\n",
    "\n",
    "This is a good start, but this is just the tip of the iceberg in terms of what could be done with a word structure model. Here are a couple ideas:\n",
    "* Easily and automatically generate plausible wrong answers for multiple-choice spelling tests. This could be useful for language learning.\n",
    "* Train the model on a fake language, and then sample words to create new words in that language.\n",
    "* Generate a name for your startup (I believe that [Wordoid](http://wordoid.com/) may already be using a Markov model!).\n",
    "\n",
    "There are also many NLP problems that could make use of word structure modeling: optical character recognition, speech recognition, swipe typing, spell checking, web search, and machine translation come to mind. I haven't found any examples of word structure modeling, perhaps because people communicate primarily using a vocabulary that can be easily stored in a dictionary. A word structure model could be useful to help to keep up with changing vocabularies, and to handle things such as names, which follow common structural patterns but are too numerous to fit into a dictionary.\n",
    "\n",
    "Additional Reading\n",
    "-----------------\n",
    "I didn't do much research on the topic before writing this, so I'm probably ignorant of a lot of great work out there. If you know of something that does word structure modeling, please let me know!\n",
    "\n",
    "* A [super fun paper](http://nl.ijs.si/janes/wp-content/uploads/2014/09/choudhuryothers07.pdf) from the Malaviya National Institute of Technology in Jaipur that uses a letter-level HMM to translate SMS abbreviations into their standard English equivalents.\n",
    "* Peter Norvig wrote a [classic blog post](http://norvig.com/spell-correct.html) on how to write an extremely simple spelling correction algorithm.\n",
    "* [Here](http://www.aclweb.org/anthology/N03-1018) is a paper on word structure modeling for OCR applications, by Kolak, Byrne, and Resnik.\n",
    "\n",
    "Thanks\n",
    "------\n",
    "Thanks to my brother [Eric](http://erickernfeld.yolasite.com/) for help with statistics, Spanish, and writing tips!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
