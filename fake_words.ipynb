{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview\n",
    "====\n",
    "\"Inatissentis sentere querum\" is a Latin expression meaning, of course, nothing, because those aren't real Latin words. In fact, they are fake Latin words generated by training an algorithm on a Latin text. These words look to me, a non-Latin-knower, like realistic Latin words, and hopefully they do to you, too. Of course, if you know Latin, they probably look terrible, and you're probably already mad at me. Instead, take a look at some of the other languages that I've modeled!\n",
    "\n",
    "This project explores using Markov chains to model word structure in an alphabet-based language, after being trained on a text in that language. This model is then used to generate realistic-looking fake words and to detect \"foreign-looking\" words.\n",
    "\n",
    "What is a \"word?\"\n",
    "----------------\n",
    "There are a lot of ways to define a \"word.\" For this project, I'll define a word to be a sequence of letters and certain approved punctuation marks. As far as approved punctuation, I have included the apostrophe in languages that use them for contractions. This means that a contraction like \"don't\" will be counted as \"don't\", rather than being counted as \"dont\" or as two separate words, \"don\" and \"t.\"\n",
    "\n",
    "For languages with accented letters, I've chosen to model accented letters as if each one were its own distinct letter. So, my French \"alphabet\" includes 'E,' 'É,' 'È,' 'Ê,' and 'Ë' as separate \"letters.\" Confusingly, it is possible for a dictionary or text to contain letters that are not in the alphabet! For example, the letters 'J,' 'K,' 'W,' 'X,' and 'Y' are not considered to be part of the Italian alphabet, but they occur frequently in loan words. I use the '?' character to model all non-alphabet characters in each language.\n",
    "\n",
    "Below are the alphabets that I've defined for each language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Just cruft. How does anyone survive without these?\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latin: a b c d e f g h i l m n o p q r s t u v w x z\n",
      "german: a ä b c d e f g h i j k l m n o ö p q r s ß t u ü v w x y z\n",
      "spanish: a á b c d e é f g h i í j k l m n ñ o ó p q r s t u ú ü v w x y z\n",
      "french: a à â ä b c ç d e é è ê ë f g h i î ï j k l m n o ö ô p q r s t u û ü ù v w x y z '\n",
      "english: a b c d e f g h i j k l m n o p q r s t u v w x y z '\n",
      "polish: a ą b c ć d e ę f g h i l ł m n ń o ó p r s ś t u w y z ź ż\n",
      "italian: a à b c d e è é f g h i ì l m n o ò p q r s t u ù v z '\n"
     ]
    }
   ],
   "source": [
    "from fake_words.fake_words import LANGUAGES\n",
    "\n",
    "for language in LANGUAGES.values():\n",
    "    print u\"{}: {}\".format(language.name, \" \".join(list(language.alphabet)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeling with a Markov chain\n",
    "===============\n",
    "What constitutes a good fake word? \"Mait\" seems like a good fake French word to me, whereas \"xkuq\" does not seem like a good fake French word. How can we differentiate between these words?\n",
    "\n",
    "There are a couple good clues in here. One important aspect is letter frequency. 'R,' 'A,' 'I,' and 'T' are all commmon letters in French, whereas 'X,' and 'K' are not. Another important thing is sequences of letters. The letter sequences \"XK,\" \"UQ,\" \"XKU,\" and \"KUQ\" are all exceedingly uncommon. It's also realatively uncommon for a word to start with \"X,\" or to end with \"Q.\"\n",
    "\n",
    "This can be used to create a simple language model wherein words with common sequences of letters are judged to be likely, and words with uncommon sequences are judged to be unlikely. I have chosen to use a [Markov chain](https://en.wikipedia.org/wiki/Markov_chain) to model the interactions between letters. Instead of modeling each letter individually, now each word is modeled as a sequence of states, where each state corresponds to a letter. This means that there is a parameter for the probability of each transition between any two states. These parameters can still be estimated from the training document. Note that this is just a vanilla discrete Markov chain where the states are the letters themselves; there is no hidden state.\n",
    "\n",
    "Unigram model\n",
    "------------\n",
    "First, I have a letter frequency model that looks at each letter individually. For this, the likelihood of a word is the product of the likelihood of each letter in the word. This can be thought of a degenerate version of a Markov model, i.e. a [Markov chain of order 0](https://cw.fel.cvut.cz/wiki/_media/courses/a6m33bin/markov-chains-2.pdf).\n",
    "\n",
    "For an *N*-letter alphabet, there are *N* parameters, which are estimated using maximum likelihood estimates from training documents.\n",
    "\n",
    "Bigram model\n",
    "-----------\n",
    "For this model, I treat the word as a simple Markov chain where each state corresponds to a single letter. There is a parameter from the probability of transitioning from any given state to any other state. For an *N*-letter alphabet, there are now *N*² state transition parameters to learn. This will create a lot of cases where the parameters aren't covered by the training data. For example, *A Tale of Two Cities* doesn't contain the letter sequences \"XZ\", \"ZX,\" or \"VF,\" amongst many others. In order to prevent this from zeroing out the likelihoods, I've used [add-one smoothing](https://en.wikipedia.org/wiki/Additive_smoothing), adding one to any [bigram](https://en.wikipedia.org/wiki/Bigram) that doesnt appear in the training text. A slightly more principled way to do this would be to add a smoothing parameter *α*, instead of 1, to each count. This parameter *α* could be learned by doing cross-validation on another text. If I weren't too lazy to do it, that is.\n",
    "\n",
    "I'm handling the starts and ends of words specially in order to make the model generate more realistic words. For example, the model should capture that it's extremely uncommon for an English word to end with the letter \"Q.\" To do this, I added Markov chain states for start and end tokens. This idea is taken from computational linguistics, where it is common to put these tokens at the start and end of sentences when modeling sentence structure. These tokens behave more or less like letters, except that they must appear at the start and the end of each word, and they may not appear in the middle of the words. So, the word \"in\" is represented as, ```[\"start token\", \"I\", \"N\", \"end token\"]```.\n",
    "\n",
    "Trigram model\n",
    "------------\n",
    "For this model, I treat the word as a [Markov chain of order 2](https://cw.fel.cvut.cz/wiki/_media/courses/a6m33bin/markov-chains-2.pdf), where each state corresponds to a single letter, but it depends on the *two* previous states, not just the one previous state. This lets the model know about common and uncommon [trigrams](https://en.wikipedia.org/wiki/Trigram).\n",
    "\n",
    "Final multiplicative model\n",
    "------------------------\n",
    "I'm using the two Markov models alongside the letter frequency model from the previous section. To calculate the proportional likelihood of a word this way, I take the likelihood of the word in each of the three models above. This results in a function that is not an actual likelihood, but is *proportional* to the likelihood, which works fine for what I'm trying to do. To get the actual likelihood, it would be necessary to divide this proportional likelihood by a very nasty constant.\n",
    "\n",
    "Multiplying these three models together provides a smoothing effect that counteracts the sparsity of training data for the bigram and trigram parameters. For example, even if the training document didn't have any instances of the trigram, \"ERN,\" we would still be able to see that its consitituent letters and bigrams are fairly common, so the sequence \"ERN\" would have a relatively high likelihood. This idea is stolen from [Eugene Charniak's](https://cs.brown.edu/~ec/) computational linguistics class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the training document for each language, and its length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latin training document: Cicero's Orations, 112249 characters\n",
      "german training document: Siddhartha, 237229 characters\n",
      "spanish training document: Belarmino y Apolonio, 450096 characters\n",
      "french training document: Les Misérables, 692101 characters\n",
      "english training document: A Tale of Two Cities, 792985 characters\n",
      "polish training document: Pan Tadeusz, 240531 characters\n",
      "italian training document: La vita italiana durante la rivoluzione francese e l'Impero, 67461 characters\n"
     ]
    }
   ],
   "source": [
    "for language in LANGUAGES.values():\n",
    "    print \"{} training document: {}, {} characters\".format(language.name, language.corpus_name, len(language.corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We're going up to trigrams\n",
    "MAX_GRAM = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from fake_words.fake_words import Language\n",
    "\n",
    "trigram_languages = [Language(info, 0, MAX_GRAM) for info in LANGUAGES.values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below prints out the highest-frequency n-grams in each language. Start and end tokens are shown with the ⋅ character. So, \"e⋅\", the most frequent English bigram, occurs when a word ends with the letter \"E.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 1-grams in latin: ⋅, e, i, t, a, u, s, n, o, r\n",
      "top 2-grams in latin: s⋅, m⋅, e⋅, t⋅, er, ⋅a, qu, is, ⋅i, ⋅c\n",
      "top 3-grams in latin: um⋅, is⋅, ⋅qu, am⋅, ⋅in, us⋅, ⋅co, em⋅, ent, que\n",
      "\n",
      "top 1-grams in german: ⋅, e, n, i, r, s, a, t, h, d\n",
      "top 2-grams in german: n⋅, en, e⋅, er, r⋅, ch, ⋅d, ⋅s, t⋅, in\n",
      "top 3-grams in german: en⋅, er⋅, ein, ich, nd⋅, ch⋅, ⋅de, und, te⋅, ⋅un\n",
      "\n",
      "top 1-grams in spanish: ⋅, e, a, o, s, n, r, i, l, d\n",
      "top 2-grams in spanish: a⋅, e⋅, o⋅, s⋅, ⋅e, ⋅d, n⋅, de, en, ⋅l\n",
      "top 3-grams in spanish: ⋅de, de⋅, os⋅, ⋅la, ⋅co, la⋅, que, ⋅qu, as⋅, es⋅\n",
      "\n",
      "top 1-grams in french: ⋅, e, a, i, t, s, n, r, u, l\n",
      "top 2-grams in french: e⋅, t⋅, s⋅, ⋅d, ⋅l, ai, es, le, ⋅c, ⋅p\n",
      "top 3-grams in french: es⋅, it⋅, ⋅de, le⋅, ait, de⋅, ⋅qu, nt⋅, ⋅le, re⋅\n",
      "\n",
      "top 1-grams in english: ⋅, e, t, a, o, n, i, h, s, r\n",
      "top 2-grams in english: e⋅, ⋅t, th, d⋅, he, ⋅a, s⋅, t⋅, ⋅h, in\n",
      "top 3-grams in english: ⋅th, the, he⋅, nd⋅, ⋅an, ed⋅, and, er⋅, ng⋅, ⋅to\n",
      "\n",
      "top 1-grams in polish: ⋅, a, i, e, o, z, ?, s, n, r\n",
      "top 2-grams in polish: ie, a⋅, e⋅, ⋅s, ⋅w, ⋅p, i⋅, ni, y⋅, ⋅?\n",
      "top 3-grams in polish: ie⋅, nie, ⋅po, ⋅na, ⋅ni, dzi, ⋅i⋅, na⋅, ⋅w⋅, rze\n",
      "\n",
      "top 1-grams in italian: ⋅, e, i, a, o, n, t, r, l, s\n",
      "top 2-grams in italian: e⋅, a⋅, o⋅, i⋅, ⋅d, ⋅a, ⋅p, ⋅s, ⋅c, er\n",
      "top 3-grams in italian: he⋅, to⋅, re⋅, ⋅di, ⋅co, la⋅, ⋅e⋅, ⋅de, ⋅in, on⋅\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "for language in trigram_languages:\n",
    "    for n in range(1, MAX_GRAM+1):\n",
    "        top_pairs = sorted(language.n_grams[n].iteritems(), key=itemgetter(1), reverse=True)[:10]\n",
    "        top_n_grams = [u\"\".join(n_gram_tuple) for n_gram_tuple, count in top_pairs]\n",
    "        top_n_grams_pretty = [n_gram.replace(u\" \", u\"⋅\") for n_gram in top_n_grams]\n",
    "        print u\"top {}-grams in {}: {}\".format(n, language.info.name, u\", \".join(top_n_grams_pretty))\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since a unigram is the same as a letter, the estimated unigram frequencies should match generally-accepted letter frequencies. The English letters with highest estimated frequencies are, in order, \"ETAONI.\" This is close to the generally-accepted [list of most frequent English letters](https://en.wikipedia.org/wiki/Letter_frequency), \"ETAOIN.\" The discrepancy may be because I'm using a slightly old text ([*A Tale of Two Cities*](http://www.gutenberg.org/files/98/98.txt)), or just because the text is too short. Overall, the letter frequency ordering looks good for most languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a short method to print out the most likely words for each language, for each length of word. Since this model calculates word likelihood as a product of letter likelihoods, shorter words will tend to have higher likelihoods. In any language, the most likely word will be the empty string.\n",
    "\n",
    "Since an *M*-letter alphabet can form $M^N$ words of length *N*, exploring the search space of all words is expensive when *N* is large. I have (slightly) optimized this search by pruning the search if the likelihood of the word I'm at is already lower than the likelihood of one of the top words that's already been found. I tried to create a heuristic pruning strategy, but it did not work well.  With my current search, though, I can only generate words of up to four letters in a sane amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_WORD_LENGTH = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latin length-1: a, e, m, s, c, o, d, t, l, f\n",
      "latin length-2: es, re, se, et, is, in, te, de, ne, am\n",
      "latin length-3: que, cum, tum, qui, con, rem, sum, dis, res, dem\n",
      "latin length-4: quam, quis, atis, quae, etis, orum, inis, atum, esse, quem\n",
      "\n",
      "german length-1: e, a, o, s, n, f, r, d, u, t\n",
      "german length-2: er, in, de, en, ge, st, se, un, es, be\n",
      "german length-3: den, der, gen, ein, sen, und, ben, hen, die, ber\n",
      "german length-4: sein, inen, eine, sten, unde, wein, lein, dein, icht, sich\n",
      "\n",
      "spanish length-1: a, y, o, e, s, n, r, u, l, d\n",
      "spanish length-2: de, la, es, se, no, en, lo, el, do, co\n",
      "spanish length-3: que, des, las, los, ela, con, cos, del, res, nos\n",
      "spanish length-4: ente, ques, dela, ento, lara, dera, dese, enta, esta, este\n",
      "\n",
      "french length-1: à, a, t, e, y, s, m, i, n, r\n",
      "french length-2: de, le, re, et, ce, se, ne, es, la, me\n",
      "french length-3: que, les, des, ent, une, res, tre, ces, ait, ses\n",
      "french length-4: lait, mait, ille, sait, fait, ques, ente, elle, vait, dent\n",
      "\n",
      "english length-1: a, i, e, o, s, d, f, n, t, '\n",
      "english length-2: he, re, in, to, th, of, an, as, it, at\n",
      "english length-3: the, and, her, his, whe, hat, ing, she, ind, was\n",
      "english length-4: ther, ithe, thed, that, then, this, here, hand, athe, thes\n",
      "\n",
      "polish length-1: i, w, a, z, o, s, e, u, c, y\n",
      "polish length-2: na, za, ni, po, do, to, że, ta, mi, ch\n",
      "polish length-3: nie, wie, sie, mie, się, cie, czy, nia, pie, szy\n",
      "polish length-4: niem, dzie, przy, prze, anie, wiem, onie, znie, niał, prza\n",
      "\n",
      "italian length-1: e, a, i, o, è, s, f, n, l, d\n",
      "italian length-2: di, la, re, no, se, le, ne, to, de, in\n",
      "italian length-3: che, the, pre, ore, con, pro, non, ine, del, ile\n",
      "italian length-4: pare, pere, dere, alla, cone, core, dele, none, alle, ante\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for language in trigram_languages:\n",
    "    for word_length in range(1, MAX_WORD_LENGTH + 1):        \n",
    "        top_words = language.top_words(word_length, 10)\n",
    "        top_words_formatted = u\", \".join(top_words)\n",
    "        print u\"{} length-{}: {}\".format(language.info.name, word_length, top_words_formatted)\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One strange fake word that I notice here is \"st,\" in fake-German. This doesn't look like a good German word to me, because it does not have any vowels. Since the model never looks at the word as a whole, it has no way of \"counting\" the vowels, consonants, or any other category of letter to ensure that they are occuring in appropriate proportions. Adding in a counting mechanism can make the model a lot more difficult to perform computation on, because it means that every letter has a dependency on every other letter. If you add a vowel at the start of the word, you may need to remove a vowel somewhere else in the word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Generating fake words, finally!\n",
    "=================\n",
    "A functional system to model word probabilities can be used for many different things. To start with, we can find fake words, which are sequences of letters that look realistic, but aren't found in a dictionary. I'll just repeat the previous exercise, but using a dictionary for each language to eliminate sequences of letters that are already real words.\n",
    "\n",
    "For each language, I have found an online dictionary. There are some discrepancies in things such as how accents are treated, and what does or doesn't count as a word. Some of these things are unavoidable and language-specific, e.g. in German the \"ß\" character is used [only after short vowels](https://en.wikipedia.org/wiki/%C3%9F#Usage_in_the_reformed_orthography_of_1996) except in Switzerland, Lichtenstein, and Namibia, and you don't even want to know what the rules were before 1996."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some latin words: mordendorum, dissentbat, condignam, commortue, consputatu, impetras\n",
      "some german words: politiker, masten, gestaltungs, höllisch, geschichtsbuch, clan\n",
      "some spanish words: privanza, hiscal, festejador, tirte, antidotario, alabarda\n",
      "some french words: gagions, trainas, renfilaient, sensibiliserez, desorienta, desincarnes\n",
      "some english words: ergogenic, arrivers, balustrades, coxless, blousier, smuggling\n",
      "some polish words: interliniujcież, silpasiastra, huckleberry'ego, niepiaskujący, benzole, żaru\n",
      "some italian words: anellaccio, universalisti, fonda, sgargarizziamo, sciupinerebbe, parlamentarismi\n"
     ]
    }
   ],
   "source": [
    "from random import choice\n",
    "\n",
    "for language in trigram_languages:\n",
    "    some_words = u\", \".join([choice(list(language.dictionary)) for _ in xrange(6)])\n",
    "    print u\"some {} words: {}\".format(language.info.name, some_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latin length-2: am, us, ta, um, ae, le, to, ia, im, pe, di, co\n",
      "latin length-3: con, dis, tem, int, ium, ine, ant, tes, tis, the, sem, ius\n",
      "latin length-4: atis, atum, pere, sent, inum, inam, inte, pris, cont, ment, inem, tent\n",
      "\n",
      "german length-2: ht, om, ot, nn, hn, rn, ln\n",
      "german length-3: ine, ind, eit, har, ere, jen, ese, ert, eie, ane, fre, wit\n",
      "german length-4: inen, unde, lein, icht, eind, dien, iner, dend, eren, aben, scht, esen\n",
      "\n",
      "spanish length-2: es, co, sa, ma, mo, an, ra, po, ba, pa, da, or\n",
      "spanish length-3: ela, cos, una, den, ino, men, eno, ena, pas, ado, tra, lla\n",
      "spanish length-4: ques, dela, ento, lara, dera, esta, endo, cona, elas, quel, quen, ella\n",
      "\n",
      "french length-2: pe, it, ge, st, is, el, ar, at, al, là, pa, er\n",
      "french length-3: ent, res, tre, ant, ine, ele, ens, ple, ons, int, che, ous\n",
      "french length-4: mait, ille, ques, vait, rent, dant, quit, dest, ette, cont, lant, quis\n",
      "\n",
      "english length-2: th, se, st, le, ve, te, ge, ce, ot, mr, ch, ke\n",
      "english length-3: whe, ing, ind, hed, ine, che, sed, wer, pre, und, ond, ded\n",
      "english length-4: ther, ithe, thed, athe, thes, thad, mand, othe, ande, thas, hing, wher\n",
      "\n",
      "polish length-2: wi, ła, wo, ło, zi, ry, zy, cy, ży, zo, ły, ża\n",
      "polish length-3: mie, nia, szy, zie, bie, rzy, rze, sta, wia, cza, sze, tał\n",
      "polish length-4: dzie, onie, znie, niał, prza, siem, świe, owie, miem, dzia, nied, powa\n",
      "\n",
      "italian length-2: ri, ra, na, an, of, er, th, is, ch, ar, uo, tm\n",
      "italian length-3: tro, gre, pri, ion, cre, ate, tri, lia, ele, dis, one, mon\n",
      "italian length-4: dere, cone, dele, tere, inte, anto, dera, gute, illa, tore, into, cona\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from math import log\n",
    "\n",
    "for language in trigram_languages:\n",
    "    for word_length in range(2, MAX_WORD_LENGTH + 1):        \n",
    "        top_words = language.top_words(word_length, 100)\n",
    "        top_nonwords = [w for w in top_words if w not in language.dictionary]\n",
    "        top_nonwords_formatted = u\", \".join(top_nonwords[:12])\n",
    "        print u\"{} length-{}: {}\".format(language.info.name, word_length, top_nonwords_formatted)\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The less familiar I am with a language, the better the fake words look."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling words progressively\n",
    "===============\n",
    "What's the fun of making fake words if they can only be four letters long? Especially with German? As I mentioned above, the challenge is that the search space of *N*-letter words is too big when *N* > 4. With Latin's measly 23-letter alphabet, there are only six million possible five-letter words. French, though, with 50 total letters, allows 312 million five-letter words! So, instead of trying to cover the entire search space of long words, why not sample?\n",
    "\n",
    "For my first sampler, I sample words progressively from left to right, beginning with the start token and sampling each successive letter conditioned on the letters to its left. It should then be possible to generate *N*-letter words by doing [rejection sampling](https://en.wikipedia.org/wiki/Rejection_sampling), i.e. throwing away all samples that are not of length *N*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latin: t, in, es, to, te, or, in, dieni, his, des, in, fori, quis, tit, it, es, it, am, en, qui\n",
      "german: er, ger, sie, ste, he, lan, ein, ich, ind, auch, nich, wein, an, er, und, gen, se, es, sie, ir\n",
      "spanish: es, co, el, no, es, ra, es, en, a, enta, que, en, no, a, in, no, co, es, se, se\n",
      "french: le, se, al, le, s, sa, il, ce, se, a, ne, le, le, ant, le, a, le, il, se, se\n",
      "english: no, his, of, of, he, me, in, a, en, the, he, e, in, the, the, se, de, he, on, the\n",
      "polish: nie, si, ły, pro, mi, i, cze, nie, nie, le, na, cza, po, wic, ro, wie, cza, ra, o, ci\n",
      "italian: ro, e, in, i, a, di, in, ner, le, pro, la, e, la, e, la, a, a, e, a, re\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIEAAACbCAYAAAD1NIsvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3X+cJVV95//XeyAgyI+M/OhGRmf8icSogILmOxoaQSLo\nDm42i4rfqBCJ31W/sqJGhv3uMrOJgSGrrouSRDHswIL8MCiyQRkRWwPhh/wUBRGFHgbCNALDICFf\nHZjP/lGnmUtP39v33qq6t6ru+/l49GNu1606VT396VPnnjrncxQRmJmZmZmZmZlZsy0Y9gWYmZmZ\nmZmZmVn53AlkZmZmZmZmZjYC3AlkZmZmZmZmZjYC3AlkZmZmZmZmZjYC3AlkZmZmZmZmZjYC3Alk\nZmZmZmZmZjYC3AlUEkn3SnpzH8e9UdKdZVyTmVlTSLpc0h93sd9mSS8exDVZNUl6uaRbJG2U9JEB\nnXNxij23s6w0kg6WtK7l+x9L+v15jnFsWm6t91ZJfy3pP3VxTF+fjWw0dFN/tTmu53rQYNthX8Co\nk7QZeGlE3AMQEVcD+w73qszMqi0ijux211IvxOrgz4CrImL/AZ/XsWeD8EycRcTv9nqMWZ9a4+4/\nDPNCrBlm6i9JpwAviYj39nL47HKsMz8FGD7fiG2oJG0z7GswK5GGfQE2dIuBn8z1hkdDmJn1xfdW\nsxpz46dkkg6U9E+SNkh6QNIZkrZN732frBL9kaTHJf37OYa03Svp45JuS2V8VdJ2w/p5rFokHSDp\n5jTN4SJJF0j6r+m9t6cpEBskXS3pVS3H3SvpzyTdBjwhaZu07ROSZuLxLEl7pmk3GyWtkbRrSxkX\nSXowlT8p6Xda3jtb0hck/e9U1rWSXjTQ/xwbKEmfknR/+n3fKekQSadIujjF5eOSbpT06lnH/Dy9\n92NJ72h5732S/lHSX0l6VNIvJL215f3vSTouvX5JisHHJD0k6auzLu8tkn6WyvlC6f8ZVhmSvgsc\nAnwxxdl5ks6U9A+SfgVMSNpO0n+TtDbVaWdK2j4df7CkdZJOlDSd7uPvbyn/OZI+I2kq1YU/mDmW\n7P7+f6dyH5J08qB/fqsOSXtJ+lqKhV9I+n/T9lMkXShpdYrR2yUd0HJc2/v8HOd4ZrqNsvbnD9Nx\nD0r6b6274tg0nomZkyT9RNIjkr6i9DlD0vGS7pb0sKRvSNqrTRlna0vbczdJl6X68BFln3Va7S9/\nprE5pFh8G3Ay8E5Jv5J0S3rv/ZLuSHXkzyX96TzltNaDc34OT+9vlvTBUWwjuhOofE8B/xF4HvB7\nwJuBDwFExMFpn1dFxC4RcXH6fvbooH8PHA68CHgN8P6Sr9lqQNJvAZcAf0cWX18F/m16bz/gK8Dx\n6b2/Bb6ZjpnxLuAI4Lcj4um07Q/JYnQf4N8A3wJOAvYAtgE+2nL85cBLgD2Bm4HzZl3iO4FTgN8G\nfgF8Ou/PbNUk6eXAh4HXRsQuwB8AU+ntZcCFwEKyGP2Gtow++zmwNB2zEvhfksZaij4IuBPYDfgr\nspiey58DV0TEbwOLgDNmvf824LVk9efRkg7v80e1momIQ4F/BD6U4uw3wLuBP4+InYFrgFXAS4FX\np3/3Bv5LSzHjwM7A84EPkHUozXSIfwbYH3gDWV37Z8DmlmOXAi8DDgP+i6R9SvgxreIkCbgMuAXY\nCzgUOEHSW9Iu/wY4H9g17ffFdFzb+3wXPg/894jYlexefdGs9x2bNuMY4C1kcbIP8P9JOgT4S+CP\nyGL2PuCCLsr6OLCO7L69J9kH+lb+TGPtBPCvZHF3YUTs3DKNexo4Mt3HjwU+lz7rzOdp2nwObzGS\nbUR3ApUsIm6JiBsicx/wJeDgWbvNN6Ty8xExHRGPkTUOugl6a743ANtExBci4umI+DpwQ3rvT4G/\niYgbU+ydC/w6HTPj8xHxzxHx65ZtZ0TEwxHxINkHp+si4kcR8Rvg62QfdgCIiP8ZEU9GxCbgvwKv\nkbRzS1lfj4ibImIzWQeR47a5nga2A35X0rYRcV9E3Jveuykivp46Gj8LPIcUhxHx9xExnV5fDNxN\n1vEzY21E/F1EBLAa2EvSnnOcfxOwWNLeEfGbiPinWe+fGhG/ioh1wPdwLI6i1vvspRFxHUCq/44H\nPhYRGyPiX4DTyDqKZvyGrNPo6Yj4FvAEsE/6YH8s8NGIWJ/q2utSnQhZg3ZFiskfAbeRNTJt9BwI\n7B4Rn05xNAWcxZY4uzoirkh13blkHZKQfWhpd5+fz2+Al0raLd2rW49zbFqrM1J78DGyB3bHAO8B\nvhIRt6U6bTnwe5JeOE9Zm8g6jV6UYvaaWe/7M4210/bzcER8K9WbRMQ/AmuAN81XYETc3MXn8JFs\nI7oTqGSSXpaGRT4oaaZy3b3HYqZbXj8J7FTYBVqdPR94YNa2mamEi4FPpKGNj0raQDZC4vkt+94/\nR5mtsfavc3y/E2R5NCSdloZkPgbcS9aobI3t9S2vHbcNFhG/IHvSsgJ4SNL5LcPG17XsF2Rx93wA\nSe/VlimLG4BX0iaGIuJf08u54uiTZPezG9JUimNnve861Fq1TrneA9gRuGmmviQbAblby/6PpM7s\nGTMxtDuwPXBPh3M59gyye/Les+7Jy8lGSsDW98vnKMtXtRft7/Pz+ROyUR0/lXR9mmbRyrFpM1rb\ng2vJ7tF7pdcApA7yR8hGSnbyV2Sjv9ekNuKnZr3vuLOeSTpCWWqJR1L9eQRdfJ7u8nP4SMakO4HK\n99dk0xlekqYq/CecTM2K8SBb34xfkP69D/iLiHhe+loYETtFxIUt++ZJSv4esuHrb05xvYQsrh3b\nIyoiLoiINwEzTwlXpX9nYnJmSsQi4J/T08QvkU3TWRgRC8mS9/YcQxHxUET8aUTsDfw/wJnysvDW\nXmvd9zBZo++VLfXlb6cpNPN5GPj/yaZQmHWyDrhn1j1514h4+zzHdbrPdxQRv4iIYyJiD+B04GuS\nduj90m0EtMbUC8k6Hv+ZrG0HgKTnknWOz/UA8RkR8UREfCIiXkI2HfzENLXMrFvP+nyS8kZ9jawe\n2yO1F79Fd+1Ffw5vw51A5dsJeDwinpT0CmD2MorrAX9YsX5cCzwt6cPKEjsfxZapNGcB/0HSQZDd\nvCUdmW7iRdiJbHrZhlTmqXilu5El6eXKEkFvRzYF4V/JpogBvFbSO1IeoI+RfWi+DnguWe6Uh9PI\nsmOBvpb1lPRHkmY+KD2Wyt3c4RAz4JnRaV8G/nsaFYSkvbvJCZCOPRv4rLKkvwskvaEl95obmjbj\nBuBXyhZkeE66Z79S0uva7D8TO53u8x1Jeo+kmSfeG8nu0TP1omPTWn041XvPI/uQfEH6er+kVytL\ndv+XZCkCOo5Ek/Q2STMd478iy436dIdDzGabBpakB4eQpRvYDng4IjZLOoIsr1Q3dqbz5/CR5U6g\n8sx8IP4E8B5Jj5Ml552dVG0FcE4aHvxHHcoxe5Y0R/sPyRKVbiCbw30Z8OuIuClt/0Ka3vAz4H2t\nh89V5DzftzqHbLTRA8CPgdk5WGy0bE+WR+WXZE8P9yCb6gBwKVmS8A1kI8j+bcoTcCdZUt3ryDrD\nXwlcPc95os3rA4HrUz37DbIcLVNz7DfX99Z88/3OP0WWpPy6NFx8DfDyLsv7BHA78EOyqRKnsaVt\n5dgzANJ0wreT5Zq4F3iIrPNxl3aHpOPa3uc7HZe8FfhJqhc/B7yzJQegY9NanU9W7/2cLDffpyPi\nu8B/JktM/gBZIud3tRzTLmZeBlypbPXFa4AvRsQP5jnGDLbEx8VkHdWPSLoxIp4ATgAuTp9p3kXW\ntpyvHJj/c/jI1oXKHmTNs5M0RfYUYTOwKSIOkrSQbMWXxWSrwBwdERvT/suB48h6f0+IiDWlXL2N\nnLQiy1lkIwY2k8XZz3AsAiDpOuCvI2L1sK+l6ZStiHUh2Q1DZCP6/jNZUk/HI9nSx2RDcN877Gtp\nMseiVYnbjOXyfb43KbfSTcC6iFiW7kvHk3XEAZwcEd9O+45cLEq6F/iTiLhq2NfSdK4brUq6HQm0\nGZiIiP0jYmYY6knAlRGxD3AV6amvpN8Bjgb2JUvadGbLcC6zvD4PXB4R+5KtZPFTRjgWJf2+pLE0\nTPx9wKuAbw/7ukZBRPws1YkHkC0t+S9kK6iNbDzacDgWrWLcZiyQ7/O5nUCWb67VZyPigPQ10wG0\nL45FK5frRquMbjuBNMe+R5Et2Uv69x3p9TLggoh4Kg3Hn73kr1lfJO0CvCkizgZIMbaR0Y7FfciW\ndt1Alm/l30VactsG6jDgF2mu/CjHow2fY9GGzW3GYvk+3ydJi4AjyUaQP+utOXY/itGMxZGZ/lIB\nrhutMrrtBArgO5J+KOkDadvYzE0oItazZZnLvXn28pUPMP9ygmbdeBFZEtmzJd0s6UuSdmSEYzEi\nvhwR4xGxS0TsN/NEywbunWRz6mGE43G2iFjpqWAD51i0YXObsUC+z+fyOeCTbN3R8RFJt0o6K6UZ\ngBGNxYh4saeCDYzrRquMbjuBlqZh5keSZZB/EyOcSMmGZlvgALIkcweQTXk4CceiDVFaCWgZWSI7\ncDzakDgWrSLcZrShk/Q2YDoibuXZI3/OBF4cEfuRLUrwmWFcn40k141WGdt2s1NEPJj+/aWkb5AN\nR5uWNBYR05LG2ZJg7QHgBS2HL0rbnkWSg9w6iojZw3XvJ0vsd2P6/u/JOoFyxSI4Hm1+c8TjjCOA\nmyLi4fS960YrlWPRqmKuWCyjzQiOR5vfrHhcCiyTdCSwA7CzpHNmjU79Mtlqa+BYtAINqm50LNp8\n2rUZ5x0JJGlHSTul188FDidbDvWbwPvTbu9jy1Jt3wTeJWk7SS8CXgrc0Oai5vw65ZRT2r7Xy1cR\n5fhahlNGm3iZBtallXAADiVL9pc7FsuOx1H63TXxWubxbuCrLd+XVjeW9bMP4//b5+vvnFWMxTL/\nz8r+ffja+y97LmW2GbuNx0H83w/7fKPwM/Z6vjli5eSIeGFEvJhsSemrIuK96YP2jD8EflxULOb9\nP8pz/LCO9bnnj8Wy68ZB/swRwdjY4nZ/FrmMjS3OfW2ui+ePxRndjAQaA76eehq3Bc6LiDWSbgQu\nknQcsJYsgzkRcYeki4A7gE3Ah2K+qzDr3keB89K0h3uAY4FtcCzaEKScVIcBf9qyeRWORxswx6JV\nRGPajOPjS5ieXtv1/itXrux637GxxaxfP9XHVVkBTpe0H9lKTVPAB6HasWiN0Ji6MasXu72UFemr\nm3K9+NkgzdsJFBH3AvvNsf1RsgbnXMecCpya++rMZomI24AD53jLsWgDFxFPAnvM2ua60QbOsWhV\n0KQ2Y1kfdLKy/WFnkCLi+8D30+u2ixVUNRat/ppUN1ozdJsYeqAmJiYqU46vpdplDEKVftaqXEvT\nfp4iyxmkYVzzoM/Z9PMN65xFKvP6y/6/8bUPvuxBGPz1D/p8za8b6xiDea85z/HDOtbnrpem142j\n0IYr6nwa1sgySYWMaut1uO4MD8WtNklE++SnZZyvKqMsrYIGGY+ORevEsWhV0fT7tCTKW6hH8+Zr\nsN64brSqaHosllc3ul4sWqdYrH0nUP+B6ECrsqY3Lq1emn5Dt/pwLFpVNP0+7U6genHdaFXR9Fh0\nJ1B9dIrFSk4HMzMzs2YbH1+CpEK+xseXDPvHMTMzM6sFdwKZmZnZwG1JvJv/q59p4WbWfJIWSLpZ\n0jfT9wslrZF0l6QrJO3asu9ySXdLulPS4cO7ajOzcrkTyMzMzMzMmugEsmW2Z5wEXBkR+wBXAcsB\nJP0O2fLc+wJHAGcqm/diZtY47gQyMzMzM7NGkbQIOBI4q2XzUcDq9Ho18I70ehlwQUQ8FRFTwN3A\nQQO6VDOzgXInkFlB+s1v4VwW9SVpV0kXp6HjP5H0+roNNS8yL4tje3iaEItmZgX7HPBJnp3Fdiwi\npgEiYj2wZ9q+N7CuZb8H0jYzs8ZxJ5BZQfrNb+FcFrX2eeDyiNgXeA3wU2o21LzIvCyO7aGqfSya\nmRVF0tuA6Yi4FehUvw10OaJ+Hrz4gYqZFW3bYV+AmVkdSdoFeFNEvB8gIp4CNko6Cjg47bYamCT7\nMP7MUHNgStLMUPPrB3zp1jCORTOzrSwFlkk6EtgB2FnSucB6SWMRMS1pHHgo7f8A8IKW4xelbXNa\nsWLFM68nJiaYmJjo6qK2PHjp3vS0++irbHJyksnJyWFfhllPFDHQDvAtJ5aiiHNnDy/7KUcM62e3\n+UkiIra660maAjYCm4FNEXGQpIXAhcBiYAo4OiI2pv2XA8cBTwEnRMSaNufLHY+OxeaaKx4lvQb4\nElnCydcANwL/EXggIha27PdoRDxP0hnAtRFxftp+FtnIjUtmlVtI3dit/uO2q9Id2wVrUiwWG3uO\ntUFrd58u8XyuG62tTvEo6WDg4xGxTNLpwCMRsUrSp4CFEXFSGiF5HvB6smlg3wFeNlfQ5YnF/uLK\n8VIng6wbB10vpnNSTt3oOC9ap1j0dDCrm83ARETsHxEzCfs85cGGYVvgAOCLEXEA8C9ksTj7DuY7\nmpXNsWhm1p3TgLdIugs4NH1PRNwBXETWmX458KGBf7o2MxsQTwezuhFbd156yoMNw/3Auoi4MX3/\n92RxN513qHm/w8ytebocZu5YtNJ5yoPVVUR8H/h+ev0ocFib/U4FTh3gpZmZDYWng1kldZgOdg/w\nGPA08LcRcZakDXmmPKT3PB3M2uoQj98Hjo+In0k6BdgxvfVov0PNPeXBOmlSLHo6WL15Oliu0h2v\nBavLFBxPB2u+usRijnPi6WD10CkWux4JJGkBWZ6B+9Oc2tx5WMz6sDQiHpS0B7AmDef1lAcblo8C\n50n6LeAe4FhgG+AiSccBa8mmJBIRd0iaGWq+CQ81t2I5Fq0y3GY0M9ua60aril6mg51A1mDcJX0/\nk4fl9PSEcTkw84RxJg/LIuBKSXMmVjPrVUQ8mP79paRvkE3vyj3lATztwbbodtpDRNwGHDjHWx5q\nbgPlWLSKcZvRzGxrrhutErqaDiZpEXA28GngxNRz+VPg4JYP3pMR8QpJJwEREavSsd8CVkTE9bPK\n9HQwa6vNCjg7Agsi4glJzwXWACvJEvv1PeUhle3pYNZWk4f2espDvTQpFj0drN46TE0svM2Y3nPd\naG21aTduD/wA2C59XRoRJ6cps8ez5aHhyRHx7XTMvKMvPB3MOhlk3ejpYNZJEdPBPgd8Eti1ZdtY\nREwDRMR6SXum7XsD17bs90DaZpbXGPB1SUEWu+dFxBpJN+IpD2ZmZlXgNqNVQkT8WtIhEfGkpG2A\nayQtTW9/NiI+27q/pH3x6Asrj+tGq4x5O4EkvQ2YjohbJU102NUVpJUqIu4F9ptju1d6MDMzGzK3\nGa1qIuLJ9HJ7stVlN6Tv53o6fhReVdZK4LrRqqabkUBLgWWSjgR2AHaWdC6wPm8eFudgsRleetbM\nzKz2SmszgtuNtkW37caUiPcm4CXA36RR4gAfkfTHZEl6P56S8VZ+9MX4+BKmp9d2vf/Y2GLWr58q\n74KsW/48baXr5fN0T0vESzqYrKJcJul04JF+87A4J5B1UselZx2LzdWkPCxznA/nvaiPJsWicwLV\n23yxWGSbMZXnutHa6iIedyHLJfkpsjQBD0dESPoLYDwiPiDpDODaiDg/HXMWcHlEXDKrrKHlBOr9\neMfaoA2ybnROIOukiJxAczkN52ExMzMzs87cZrShiojHJf0D8LqI+H7LW18GLkuvPfrCepZzNoPr\nRhuKnkYCFXpijwSyDjwSyKqkSaMv5jgfftpdH02KRY8Eqrc63qd7PB+uG+ujzepguwObImKjpB2A\nK8hWlf1JRKxP+3wMODAijhnE6AuPBGq+Qd+nyyq73VRCjwSqj7JGApmZjTRJU8BGYDNZQ/MgSQuB\nC4HFwBRwdMo10NXSs2b9cCyamW1lL2C1sk+tC4BzI+K7ks6RtB9ZfTkFfBA8+sLqqpwQnZ4eWB+/\nDYFHAlkl1fEJo2OxudrFo6R7gNdGxIaWbavI5nef3mZ+94GkpWepwPxuP+2ulybFokcC1Vsd79M9\nng/XjfVRl1GSHgnUfIMfCTTYesojgeqjUywuGPTFmJk1yMzTxVZHAavT69XAO9LrZaSlZyNiCphZ\netasCI5FMzMzM5uXO4HMzPoXwHck/VDSB9K2sYiYBkg5B/ZM2/cG1rUcW7mlZ63WHItmZmZmNi/n\nBDIz69/SiHhQ0h7AGkl3sfUYWY9ttUFwLJqZmZnZvNwJZLUjaQFwI3B/RCxz8lMbloh4MP37S0nf\nIJtSMy1pLCKmJY0DD6XdvfSs9azbpWcdi1a2nMsgmw2cpO2BHwDbpa9LI+JktxvNbNQ5MbRVUqdE\nVmk5z9cCu6ROoFzJT1OZTgxtbbVZenZHYEFEPCHpucAasqVnDwUejYhVbeKxtKVn++Hkp/XSpFh0\nYuh6c2LoXKU7XgvWIWn+jhHxpKRtgGuAj5PlRRtK0nwnhm4+J4Yu9nzWPyeGtsaQtAg4EjirZbOT\nn9owjAFXS7oFuA64LD0xXAW8JU3HORQ4DbKlZ4GZpWcvx0vPWnEci2Zmc4iIJ9PL7ck+92zA7UYz\nG3GeDmZ18zngk8CuLduelfxUUmvy02tb9nPyUytMRNwL7DfH9keBw9occypwasmXZiPGsWhmNreU\nQuAm4CXA30TEHTPTZMHtRjMbTR4JZLUh6W3AdETcSrYccjt+om1mZmY24iJic0TsTza9602SJnDS\nfDMbcR4JZHWyFFgm6UhgB2BnSecC6/MmPwUnQLUtnADVzMysOSLicUmXA6/DSfOtQG4zWh05MbRV\n0nxJ1SQdDHw8JYY+nSzBX1/JT1N5TgxtbQ06yZ+Tn1o7TYpFJ4auNyeGzlX6VvE6Pr6E6em1pZxt\nbGwx69dPlVJ2VbRJmr87sCkiNkraAbiCLGn+4Qwpab4TQzefE0MXez7rX6dY9Egga4LTgIskHQes\nBY6GLPmppJnkp5tw8lMzMzOroKwDqJwmyvT0wPrqqmYvYLWyT60LgHMj4rspib7bjWY2suYdCSRp\ne+AHwHbp69KIOFnSQuBCYDEwBRwdERvTMcuB44CngBPSKiWzy/VIIGurjk8YHYvN1aTRF3OcD48E\nqo8mxaJHAtVbm5EXpbQZ036NrhtdF+dTl7rRI4Gab5B1o0cCWSed6sWupoNJ2jEinpS0DXAN8HGy\nZRQfiYjT2wylPJBsLu2VFDyUclY5+IN387gTyKqkLo3LPs+HP3jUR5Ni0Z1A9dYuFstoM6ZyG103\nui7Opy51ozuBmm+QdaM7gayTTvViV6uDRcST6eX26ZgNwFHA6rR9NfCO9HoZcEFEPBURU8DdwEH9\nXbqZmZmZ1YXbjGZmW3PdaFXSVSeQpAVp/ux6YDIi7gDGImIaICLWA3um3fcG1rUc/kDaZmbWKKlu\nvFnSN9P3CyWtkXSXpCsk7dqy73JJd0u6U9Lhw7tqayrHo1WB24xmZltz3WhV0u1IoM0RsT/ZcLQ3\nSZpg63FgHr9lZqPmBLIEkjNOAq6MiH2Aq4DlAGlY79HAvsARwJkpUaVZkRyPNnRuM5qZbc11o1VJ\nT6uDRcTjki4HXgdMSxqLiGlJ48BDabcHgBe0HLYobdvKihUrnnk9MTHBxMREL5djDTI5Ocnk5OSw\nL8Osa5IWAUcCnwZOTJuPAg5Or1cDk2QfxJ8Z1gtMSZoZ1nv9IK/ZmsvxaFVTdJsR3G60LbppN6Z6\n8RxgDNgMfCkizpB0CnA8W+Lw5Ij4djqmq0TlZv0qvm5c0fJ6In3ZKOrl83Q3q4PtDmyKiI2SdgCu\nAFYChwOPRsSqNomsXk82bO07ODG09ciJoa1K2qz0cDHZB+5dgY9HxDJJGyJiYcs+j0bE8ySdAVwb\nEeen7WcBl0fEJXOcq9HJTy2fDgknC49HJ4a2TtrUi6W0GVPZja4bXRfn0yYex4HxiLhV0k7ATWSd\n4+8EfhURn521/77A+ZS4uI0TQzffIOtGJ4a2Tjp9nu5mJNBewOo0VHwBcG5EfDfNabxI0nHAWrKh\n5UTEHZIuIhuSvgn40EDv2mZmJZP0NmA6NSwnOuzqus9K53i0CnGb0Soj5VhZn14/IelOtuRVmeuD\n0VF4lKSVw3WjVcq8nUARcTtwwBzbHwUOa3PMqcCpua/ObASNjy9henptT8eMjS1m/fqpci7I5rIU\nWCbpSGAHYGdJ5wLrPeXBitTl0N7S4tGxaDO6iUW3Ga2qJC0B9iPr0Hkj8BFJfwzcSDZ6ciNZB9G1\nLYc5Ga8VwnWjVc2808FKO7Gng1kHozwdLO9QYStep3iUdDBbpt+cDjziKQ/PlO64LNh8dWOR8ejp\nYNZJHe/TPZ4PTwerj3nu0zuR5UT784i4VNIewMMREZL+gmzK2AcGMVXW08Gab5B1o6eDWSd5p4OZ\nmVl3TsPDeq06HI9mNtIkbQt8jWz6zaUAEfHLll2+DFyWXnuUpPXMi9tYHXkkkFVSm6Rq2wM/ALZL\nX5dGxMmSFgIXAouBKeDoNKy361UePBLIOhn0U50mP+22fJoUix4JVG8eCZSrdI8EKliHpPnnkI36\nObFl23jKF4SkjwEHRsQxgxgl6ZFAzeeRQMWez/rnkUDWCBHxa0mHRMSTkrYBrpG0lGy54ysj4vQ0\n5WE5MDPl4WhgX9IqD5LmnIJjZmZmZs2R2ojvAW5PCXgDOBk4RtJ+ZMvGTwEfBI+SNLPR4ZFAVkld\n5L3YkWx+9/uBS4CDW5KfTkbEKySdBERErErHfAtYERFbrfLgkUDWSZNGX8xxPvz0uT6aFIseCVRv\nHgmUq3SPBCpYXepGjwRqPo8EKvZ81r9Osbhg0BdjloekBelpznqyzp47gLGImIZnlgPdM+2+N7Cu\n5XCv8mBmZmZmZmYjy9PBrFYiYjOwv6RdgCskTbB1d7S7kc3MzMzMzMxmcSeQ1VJEPC7pcuB1wLSk\nsZbpYA+l3bpe5QG80oNt4ZUezMzMzMysiZwTyCqpzepguwObImKjpB2AK4CVwOHAoxGxKiWGXhgR\nM4mh512J8pDBAAAXYklEQVTlIZXtnEDWVl1yDfR5PpyHoj6aFIvOCVRvzgmUq3TnBCpYm3bjIuAc\nYIwsCfSXI+J/5F1V1jmBrBPnBCr2fNY/rw5mTbEXsFpZ7bMAODcivptyBF0k6ThgLdmKYF7lwczM\nzGx0PQWcGBG3StoJuEnSGuBYvKqsmY0wjwSySqrjE0aPBGquNk8Ytwd+AGyXvi6NiJOH+YSxH376\nXC9NikWPBKq3Ot6nezwfHglUH93Eo6RvAF9IX32vKuuRQNaJRwIVez7rn1cHMzMrWET8GjgkIvYH\nXg28WdJS4CSyJ4z7AFeRPWFk1hPGI4Az06g2s1wci2ZmnUlaAuwHXIdXlTWzEedOIDOzPkXEk+nl\n9mT16QbgKGB12r4aeEd6vQy4ICKeiogp4G7goMFdrTWZY9HMbG5pKtjXyEY9PoFXlTWzETdvTqCy\nkqpVyfj4Eqan1/Z83NjYYtavnyr+gsysFiQtAG4CXgL8TcpD9awnjJJanzBe23K4nzBaYRyLVgWj\n0Ga0epG0LVkH0LkRcWnanHtVWa8oazO6WVHWdaNVzbw5gVLlON6aVI3s6eKxwCMtSdVmr8h0ICmp\nGnOsyFSlnEDOK1Q9dcw14JxAzTVfPErahWy1uuXAJRHxvJb3HomI3SSdAVwbEeen7WcBl0fEJbPK\nanTeC8unSbHonED11iY/VSltxlR2o+tG18X5tKsbJZ0DPBwRJ7ZsW0WOVWWdE8g6GWTd6JxA1kmu\n1cHSXNn16fUTku4kC8ajgIPTbquBSbL8A88MMwemJM0MM78eM7MGiojHJV0OvA4/YbQCdfOEsZVj\n0crSTSy6zWhVknKjvQe4XdlKsgGcDKzCq8raALlutKrpaXWwlFRtEvhdYF1ELGx579GIeN6gnzB6\nJFAzeSSQRwJVSZunOrsDmyJio6QdyEZfrAQOZ0hPGPvhp8/10qRY9EigeutiVNoSCmozpvcaXTe6\nLs5nkO1GjwSyTgZZN3okkHWSayRQSyHPSqqWBd2z+LdmZqNkL2C1srvhArJ8A99NTxv9hNEGybFo\nleI2o5nZ1lw3WlV01QkkJ1WzkvU65cFs2CLiduCAObY/ChzW5phTgVNLvjQbMY5Fq5Ky2ozgdqNt\n4Xaj1U15deOKltcT6ctGUS/1YlfTwaqWVG1WOXg6WPN4Oping1VJXYaZ93k+PAWhPpoUi54OVm+D\nTMSbymh03ei6OJ+61I2eDtZ8g6wbPR3MOulUL3azOthS4AfA7WS/8ZmkajcAF5H1Uq4lW9LusXTM\ncuBPyIaZz7mknTuBrBN3ArkTqErq0rjs83z4g0d9NCkW3QlUb23yU5XSZkz7NbpudF2cT13qxmF2\nAo2PL2F6em1PZx4bW8z69VM9HTPqBlk3uhPIOsnVCVQWdwJZJ20q0EXAOcAYsBn4ckT8D0kLgQuB\nxcAUWQW6MR2zHDgOeIqSG5fuBGquQTcuyyp7rsacP3jUS10+6HRZPu4Eqq86Pqzp8Xy4E6g+2rQb\nvwK8HZiOiFenbacAx7Nl2s3JEfHt9F7pbcZhdgK5fTkYg28zuhPI5tYpFhcM+mJaSer5a3x8yTAv\n2YbrKeDEiHgl8HvAhyW9gmwpxSsjYh/gKmA5QBpKeTSwL3AEcKaymsus4qKUr16fAJqZmdXY2cAf\nzLH9sxFxQPqa6QDaF7cZzWxEDLUTyB9irBcRsT4ibk2vnwDuJEuUdhSwOu22GnhHer0MuCAinoqI\nKeBu4KCBXrSZmZmZDVxEXA1smOOtuTp3jsJtRjMbEUPuBDLrj6QlwH7AdcBYRExD1lEE7Jl22xtY\n13LYA2mbmZmZmY2mj0i6VdJZknZN29xmNLOR4U4gqx1JO5EtsXhCGhE0ewKpJ5SamZmZ2WxnAi+O\niP2A9cBnhnw9ZmYDt+2wL8CsF5K2JesAOjciLk2bpyWNRcS0pHG2JPt7gCzb/oxFaducVqxY8czr\niYkJJiYmCrxyq5PJyUkmJyc77lNmonKzXjgWzcy6ExG/bPn2y8Bl6bXbjNaXbtqMZlUz1NXBhreS\n0rPL8epg1dMum7mkc4CHI+LElm2rgEcjYpWkTwELI+KklBj6POD1ZEN6vwO8bK4lHbw6mHXSZtWR\ncWA8Im5No9NuIsspcCzwSESc3iYeDyRrXF7JHPE46JUevCJNvQw6Fr06mLXj1cFyle66uGAd2o1L\ngMsi4lXp+/GUOgBJHwMOjIhjBtVm9OpgzefVwYo9n/WvUyx6JJDVhqSlwHuA2yXdQlYDnQysAi6S\ndBywlmx1ByLiDkkXAXcAm4APDbQFaY2WGpHr0+snJLUmKj847bYamCRbwe6ZROXAlKSZpJPXD/jS\nrWEci2ZmW5N0PjAB7CbpPuAU4BBJ+5GNmpwCPghuM5rZaHEnkNVGRFwDbNPm7cPaHHMqcGppF2VG\n50TlkloTlV/bcpiTTlrhHItmZpmIOGaOzWd32N9tRjMbCU4MbWaWgxOVW1U4Fs3MzMxsPh4JZGbW\np/ISla9oeT2RvmwUdZtwsqxYdPJTm+Hkp2ZmZs3gxNAFlWHFqmPCSSeGbq5BJip3YmjrZNCx6MTQ\n1k4d79M9ng/XxfUx6GS8Tgxt7TgxdLHns/51ikVPBzMz60NLovI3S7pF0s2S3kqWqPwtku4CDgVO\ngyzpJDCTdPJynHTSCuJYNDPbmqSvSJqW9KOWbQslrZF0l6QrJO3a8t5ySXdLulPS4cO5ajOz8nkk\nUEFlWLHq+ITRI4Gaq8lPdfz0uV7q8rS7y/LxSKD6GsZ9uoxyx8YWs3791Fznw3VxfcwVj5LeCDwB\nnBMRr07bVgGPRMTpbUZIHkg2RfZKvEQ8oxA7RWtymzGdE48EqodcI4Hci25mZmZm3Si33RiFf01P\nry3mB7fKiYirgQ2zNh8FrE6vVwPvSK+XARdExFMRMQXcDRw0iOu05vPnaauabqaDnQ38waxtJwFX\nRsQ+wFXAcoDUi340sC9wBHCmsu5CMzMzM2s+txutyvaMiGmAiFgP7Jm27w2sa9nvgbTNrAiuF61S\n5u0Eci+6mZmZmXXD7UarGc8/sdK5XrSq6XeJ+Gf1oktq7UW/tmU/96KbmZnZQI2PLylsmk+7nDHW\nE7cbrSqmJY1FxLSkceChtP0B4AUt+y1K2+a0YsWKZ15PTEwwMTFR/JVaLUxOTjI5OdnPoa4XbWj6\n7QSazb3oNhCSvgK8HZhuSfK3ELgQWAxMAUdHxMb03nLgOOAp4ISIWDOM6zYzs8HJOoCKaZpMT3sU\nfgncbrRBUfqa8U3g/WSrJ74PuLRl+3mSPkf2gfulwA3tCm3tBLLRNrsTcOXKlf0W5XrRBqbfTqBC\netFhRcvrifRlo6iHXvSzgTOAc1q2zcypnVnpYTlw0qw5tYuAKyXNudKDmZmZlcbtRitUN+1GSeeT\nBcluku4DTgFOAy6WdBywlqydSETcIeki4A5gE/AhtxetZK4XrVC9jErraol4SUuAyyLiVen7VcCj\nEbGqzfKKryfrRf8OHZZX9BLx1k6nJe0kLSaLx5mRQD8FDm6pRCcj4hWSTgIiIlal/b4FrIiI6+co\n00vEW1tNXu7TyxLXi5eIb1taQW2D+cu2zDz36SVUpt04708y4GWQ5z6n6+J86lI3eon45msXi/Wq\nF8FLxNdf3iXizwf+CXi5pPskHUvWi/4WSXcBh6bviYg7gJle9MtxL7oNhld6sIHzcp9WFY5FqxK3\nG83Mns31Yv/Gx5cgqZSv8fElw/7xhqarkUClnNgjgayDHkcCPRoRz2t5/5GI2E3SGcC1EXF+2n4W\ncHlEXDJHmR4JZG3NFY+S3gg8AZzTEourgEdapibOfqpzIGlqIhV5quOnz/Uy6Fj0SKCty7bMIEde\npPN5JFCB52sajwQq49hnH2/dafLo8XROmlwXN0mnWCwqMbTZMHmlBytUN3NqI+Lq1CHZ6ijg4PR6\nNTBJlrPqmeU+gSlJM8t9bjU10axXZcZi1vjKzytsmZmZmVWDO4GsjrzSg5Uqx0oPXu7TqqKgWPQK\nW2bWPJKmgI3AZmBTRBykDqvNmpk1ybw5gax7/c5ZHOX5iL3ynFqrGcebVYVj0cxsi83ARETsHxEH\npW0zq83uA1xFttqsmVnjeCRQgaan19JPO9tPSLsXEce0eeuwNvufCpxa3hWZPYuX+7RC9bLc5yyO\nRStUjlg0qyKx9cPwdtNozcwaxYmhCyqjyHJsOAknnRja2mkXj2rAcp9OuFcv9Y3FsmPPiaEHzYmh\niz2n6+J8eo1HSfcAjwFPA38bEWdJ2hARC1v2edbCIy3bnRja2nJi6Lqcr/05m8KJoc3MCpamJk4A\nu0m6DziFbCrixZKOA9YCR0M2NVHSzNTETXhqohXIsWhm1rOlEfGgpD2ANSmlwOy60HWjmTWSRwIV\nVEaR5ZhHAvlJTbU0+anOMJ6wjI8vSdNnizUKK1DVNxY9EqhpPBKo2HP6aXc+eeJR0inAE8AHyPIE\nzUyj/V5E7DvH/nHKKac8830vK8p6JFDzzJ4qu3Llyprep7cq3SOBaq5TvehOoILKKLIccydQ3pt0\nvx+yR+FDdD/q+8F7q9Ir8cFj0A2IJqlvLLoTqGncCVTsOf1BJ59e4lHSjsCCiHhC0nOBNcBKssVF\ntppGO8fxng5mbdX3Pr1V6ZVow7lu7F+nWPTqYGYNtCVJeW9fZYzOMDMzs+rpd1Xb+b5qsOrtGHC1\npFuA68jyqa0BVjHHarOW6TVeahAHZiPLI4EKKqPIcswjgfI+qXEsFqvJT3U8Eqhe6huLHgnUNB4J\nVOw5XRfnPOOA68ZRHAmU59yjpL736a1Kr0S94ZFA/fNIIDMzM7OCFDWCwk/KzczMbNC8OpiZmZlZ\nD7ZMuc1bzsAG0piZmZkBHglkZmZmZmZmZjYS3AlUMf0OMfeQ8vz8/25mZmZmZmZNVlonkKS3Svqp\npJ+lZRatC17VqXjdx6L/361crheHo6wVcOreEex4tKpwLFpVOBbL0c99uM731yI4FodjVNqMpXQC\nSVoAfAH4A+CVwLslvaL7EiYLupIiyimijKLKKaIMmJzMX05VyphP/liEKv3uqnItRf3uqhRHZcdj\nMbE422Tu66r+OfOfr7fO9e/1sG8xHcGDqAtnKzYeJwu7rsGWXXb5ZZZdbtwMMiabUTcO+nzDOOdg\nz1fHejH/Nec5fljHdnd8+/tw+3tuN/fXPP/neX9fZcZoOfUiNL3eGHybsbd2Y5XajGWNBDoIuDsi\n1kbEJuAC4KjuD58s6DKKKKeIMooqp/syOvViHnLIIbl7KKv04X0eOWMRRjWOyo4hqFYcDSAeC4jF\n2SbzX1Xlz9n08w3nww6FxuNkcVc10LLLLr+/srt9CtmpHs771HHAMdmAunHQ5xvGOQd7vjrWi+4E\nGvy5m9oJRCn1IjS93hiFurjqnUB7A+tavr8/bbMB6dyLeUrb9xo4vcmx2CfHUOEci1YljseK6v4p\nZPt6uGb1sWPRqsKxaFXhWBwhvUxBW7lyZdf7duLE0NZWp4DsFIBVmu9ow1dEHDkWrU56nU9e1A3d\nrBvdxGc3MWnWq7I+6DgerZN2cef2o1VFb1PQ5n8AtOWrPUV03qEfkt4ArIiIt6bvTwIiIla17FP8\nia1RIiL3Xb2bWEzbHY/WUd54dCxaUQZVNzoWbT6+T1uV+D5tVeFYtKpoF4tldQJtA9wFHAo8CNwA\nvDsi7iz8ZGYdOBatKhyLViWOR6sKx6JVhWPRqsKxaGXbtoxCI+JpSR8B1pBNOfuKg9aGwbFoVeFY\ntCpxPFpVOBatKhyLVhWORStbKSOBzMzMzMzMzMysWiqVGFrSVyRNS/pRjjIWSbpK0k8k3S7po32W\ns72k6yXdksr6yxzXtEDSzZK+maOMKUm3peu5oc8ydpV0saQ708/0+j7KeHm6hpvTvxv7+T+WtDxd\nw48knSdpu17LKFtV4tGx2LaMkYnF2YqIzR7PV0i92uM5C4v7Hs+b+2+kh3Pl/lsapjLjsMyYG0Rs\nlRlHZcdNEfXzsDS9bhyFejGdr3Z1o6S3SvqppJ9J+lQPx+WK2TwxWEQ85YmNPL/nPPVU3vaj243z\nnsttxvLOV1zdGBGV+QLeCOwH/ChHGePAfun1TmTzKV/RZ1k7pn+3Aa4DlvZZzseA/wV8M8fPdQ+w\nMOf/7/8Ejk2vtwV2yVneAuCfgRf0eNzi9PNsl76/EHjvMGJunuusTDw6Fkc7Fue47tyx2eP5CqtX\nezxvIXHf4zlz/430cK7cf0vD/CozDsuOubJjq8w4Kjtuiq6fB/k1CnVj0+vFdL5a1Y2pDfLz1Kb4\nLeDWbuMgb8zmjcG88ZQnNvL8nouqp3ptP7rdWH5M5jiv68Yevio1EigirgY25CxjfUTcml4/AdwJ\n7N1nWU+ml9uTVRI9X5ukRcCRwFn9XENrUeQYuSVpF+BNEXE2QEQ8FRGP57ymw4BfRMS6Ho97HPgN\n8FxJ2wI7klXAlVKleHQszqvRsThbEbHZ4/kKq1d7PG/uuO9FgX8jXZ+Sio3I7UWZcVh2zJUZWwOI\no9LipqT6eWBGoW4cgXoR6lc3HgTcHRFrI2ITcAFwVDcH5o3ZvDGYJ54KiI2+fs8F11O9th/dbpz/\nXG4zlnhaCqob61TB9kzSErJez+v7PH6BpFuA9cBkRNzRRzGfAz4J5E2+FMB3JP1Q0vF9HP8i4GFJ\nZ6dha1+StEPOa3on8NVeD4qIDcBngPuAB4DHIuLKnNdSeXni0bE4L8figOStV3s8VxFx34ui/ka6\nlfdvaSSUEXMlx1bZcVRm3JRRP4+EQdWNI1AvQv3qxr2B1k6E+xnAh97Z+onBnPGUNzb6/T0XWU/1\n1H50u7E3bjMWrrC6sbGdQJJ2Ar4GnJB6IXsWEZsjYn9gEfD7kg7u8RreBkyn3lClr34tjYgDyHoc\nPyzpjT0evy1wAPDFVM6TwEn9Xoyk3wKWARf3ceyLyYbPLQaeD+wk6Zh+r6UO8sajY7E9x+LgFFGv\n9iJv3Pei4L+RbuX9W2q8smKurNgaUByVGTeF1s+jYpB14wjUi+C6sWf9xmC/8VRQbPT7ey6knuqn\n/eh2Y/fcZixFYXVjIzuB0vC8rwHnRsSlectLQwz/AXhdj4cuBZZJuoesl/kQSef0eQ0Ppn9/CXyd\nbOhpL+4H1kXEjen7r5FVoP06ArgpXU+vXgdcExGPRsTTwCXA/5XjWiqtyHh0LM7JsTgARdervcgR\n970o7G+kWwX8LTXaIGKuhNgqPY5Kjpui6+fGG1bd2NR6EWpZNz4AvLDl+0Vp20AUEYN9xFPu2Mjx\ney6qnuqn/eh2YxfcZixHkXVjFTuBiuhN+zvgjoj4fN8XIe0uadf0egfgLWSJ3roWESdHxAsj4sXA\nu4CrIuK9fVzLjqk3FUnPBQ4HftzjtUwD6yS9PG06FMgzTO7d9DH9JrkLeIOk50hSupY7c1xLmYYe\nj47FeY1KLM42yCcPUEC92osi4r4XRf2NdKuIv6WKKDMOS4m5MmOr7DgqO25KqJ+HobF1Y9PrRaht\n3fhD4KWSFitbKepdQC+rBeWN2b5iME885Y2NPL/nAuupftqPbjd2x23GghVdN1aqE0jS+cA/AS+X\ndJ+kY/soYynwHuDN2rL831v7uJy9gO8pm1t4HVnW7+/2UU4RxoCrW67lsohY00c5HwXOk3Qr8Bqg\nr+XzJO1Ilkjtkn6Oj4jbgHOAm4DbyCqkL/VTVpkqFI+OxTZGJRZnKyI2ezxfUfVqL6oU92Uo6m9p\naMqMw5Jjrs6xNYi4KaR+HoYRqBvrHLvdql3dmEaEfARYA/wEuCAiuuoYyBuzOWNwmPGU9/ecq57q\nt/3odmNX53KbsRyF1o2KGGQuIzMzMzMzMzMzG4ZKjQQyMzMzMzMzM7NyuBPIzMzMzMzMzGwEuBPI\nzMzMzMzMzGwEuBPIzMzMzMzMzGwEuBPIzMzMzMzMzGwEuBPIzMzMzMzMzGwEuBPIzMzMzMzMzGwE\nuBPIzMzMzMzMzGwE/B+4O+9E+uJx6AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13310a850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(trigram_languages))\n",
    "fig.set_figheight(2)\n",
    "fig.set_figwidth(20)\n",
    "\n",
    "for language, axis in zip(trigram_languages, axes):\n",
    "    progressive_samples = language.sample_progressive(1000)\n",
    "    word_lengths, counts = np.unique([len(word) for word in progressive_samples], return_counts=True)\n",
    "    \n",
    "    axis.set_xticks(word_lengths)\n",
    "    axis.set_title(language.info.name)\n",
    "    axis.bar(word_lengths, counts, align='center')\n",
    "    \n",
    "    print u\"{}: {}\".format(language.info.name, u\", \".join(progressive_samples[:20]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, this sampler is spending a lot of time generating short words, and also many repeated words. I suspect that the distribution of word length has an exponential tail. This means that generating long words with rejection sampling will be tragically inefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling words with MCMC\n",
    "=============\n",
    "How can we cover our sample space more effectively? This question always seems to have the same answer: [Markov chain Monte Carlo](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo). Note that the \"Markov chain\" in \"Markov chain Monte Carlo\" is a Markov chain where each state is a *sampled word,* and the chain is a series of samples. This is different than the Markov chain used to model word likelihood, which is a Markov chain where each state is a *letter,* and the chain of letters forms a word.\n",
    "\n",
    "As such, I made a [Metropolis-Hastings sampler](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm). I'm initializing each run with an *N*-letter word comprised of randomly uniformly chosen letters. The proposal distribution makes a 50/50 random choice between the following two options: it can replace a single letter of the previous sample with a randomly uniformly chosen letter, or it can swap two letters. Note that both of these moves keep the word at length *N*.\n",
    "\n",
    "Below, the \"sample\" words represent samples, taken from the sampler at regular intervals (not sequentially, in order to reduce autocorrelation). They are presented in the order that they were sampled. The \"top\" words represent the highest-likelihood words that the sampler finds. They are presented with highest likelihood first. I restart the sampler from random initializations many times, in case it tends to get stuck in certain areas of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-letter latin samples: a, a, a, a, a, a\n",
      "1-letter latin top: a, e, m, s, c, o\n",
      "\n",
      "2-letter latin samples: se, es, se, at, es, me\n",
      "2-letter latin top: es, re, se, et, is, in\n",
      "\n",
      "3-letter latin samples: con, que, aes, his, ete, mum\n",
      "3-letter latin top: que, cum, tum, qui, con, rem\n",
      "\n",
      "4-letter latin samples: pere, quis, tera, idis, itur, orum\n",
      "4-letter latin top: quam, quis, atis, quae, etis, orum\n",
      "\n",
      "5-letter latin samples: tutam, milis, verum, terum, catis, potem\n",
      "5-letter latin top: conis, quium, estis, intis, perum, intum\n",
      "\n",
      "6-letter latin samples: inatum, gantas, ominem, pratis, ventem, coniss\n",
      "6-letter latin top: querum, sentis, conium, inatis, sentum, queris\n",
      "\n",
      "7-letter latin samples: licerit, eticaus, etistis, consect, ititant, leritam\n",
      "7-letter latin top: interum, estatis, interis, peratis, sentere, sentiam\n",
      "\n",
      "8-letter latin samples: conterit, destitam, estatule, ataticut, estiriam, quenitis\n",
      "8-letter latin top: quitatis, essentis, senterum, essentum, intentis, senteris\n",
      "\n",
      "9-letter latin samples: tulatquae, itecentis, inestisse, hatemorum, omalentum, ressentam\n",
      "9-letter latin top: sententis, sententum, quitatiam, interatis, ressentis, peritatis\n",
      "\n",
      "10-letter latin samples: intinentum, stillarest, dermsicent, constantis, dereinatus, oreponsiti\n",
      "10-letter latin top: quissentis, quissentum, queritatis, senteratis, quissentem, intenteris\n",
      "\n",
      "11-letter latin samples: sultesserae, thererentes, ressectatis, quitintione, volerestius, vediciverem\n",
      "11-letter latin top: sententerum, essententum, intententis, sententeris, quissentere, interitatis\n",
      "\n",
      "12-letter latin samples: itarenticonc, comincoritis, tatenbenitam, densultentia, catullecites, mentessentes\n",
      "12-letter latin top: senteritatis, sentententum, contententis, essententere, sentententem, contententum\n",
      "\n",
      "\n",
      "1-letter german samples: e, o, a, a, e, e\n",
      "1-letter german top: e, a, o, s, n, f\n",
      "\n",
      "2-letter german samples: er, er, er, an, er, de\n",
      "2-letter german top: er, in, de, en, ge, st\n",
      "\n",
      "3-letter german samples: ein, den, gen, ang, den, ere\n",
      "3-letter german top: den, der, gen, ein, sen, und\n",
      "\n",
      "4-letter german samples: mein, unde, sein, dier, sein, iner\n",
      "4-letter german top: sein, inen, eine, sten, unde, wein\n",
      "\n",
      "5-letter german samples: einen, einen, jenen, sicht, under, ereit\n",
      "5-letter german top: unden, einen, under, ichen, schen, einer\n",
      "\n",
      "6-letter german samples: vinder, serwer, unders, denden, seinen, warden\n",
      "6-letter german top: seinen, einden, denden, sichen, nichen, einder\n",
      "\n",
      "7-letter german samples: meinene, seingen, naheich, diesein, abenden, denenst\n",
      "7-letter german top: seinden, undenen, sichten, seinder, einenen, eseinen\n",
      "\n",
      "8-letter german samples: eineinen, undendes, indender, einender, gendende, dendesen\n",
      "8-letter german top: undenden, einenden, undender, icheinen, einender, undeinen\n",
      "\n",
      "9-letter german samples: seingenen, siebessen, bieseiter, unenengen, undeinden, wenkeinen\n",
      "9-letter german top: seinenden, sicheinen, nicheinen, seinender, eindenden, dendenden\n",
      "\n",
      "10-letter german samples: ineneinder, deseindein, dereintenn, undeleiche, eseneinein, niehendein\n",
      "10-letter german top: undeseinen, eineseinen, undereinen, seindenden, sicheinden, undenenden\n",
      "\n",
      "11-letter german samples: gesamandend, undendesete, eineinenein, ichtendesen, bineseinden, gedernstere\n",
      "11-letter german top: undendenden, seineseinen, undendender, icheinenden, undeseinden, undendeinen\n",
      "\n",
      "12-letter german samples: deseinendend, beinendarcht, abesendendie, seinennichen, eindeinender, dandeseseder\n",
      "12-letter german top: seinendenden, sicheinenden, seinendender, dendendenden, seineinenden, dieseinenden\n",
      "\n",
      "\n",
      "1-letter spanish samples: a, a, a, a, a, a\n",
      "1-letter spanish top: a, y, o, e, s, n\n",
      "\n",
      "2-letter spanish samples: ta, en, la, es, de, de\n",
      "2-letter spanish top: de, la, es, se, no, en\n",
      "\n",
      "3-letter spanish samples: uno, des, los, eno, des, per\n",
      "3-letter spanish top: que, des, las, los, ela, con\n",
      "\n",
      "4-letter spanish samples: ento, lase, dese, quel, dela, sera\n",
      "4-letter spanish top: ente, ques, dela, ento, lara, dera\n",
      "\n",
      "5-letter spanish samples: estos, dento, quede, lesta, denta, sente\n",
      "5-letter spanish top: quera, dente, quela, desta, deste, entes\n",
      "\n",
      "6-letter spanish samples: comene, anosas, larato, queden, mentes, elidel\n",
      "6-letter spanish top: entera, quente, questa, queste, destra, queras\n",
      "\n",
      "7-letter spanish samples: mentere, destese, corquel, trarano, entenue, desiera\n",
      "7-letter spanish top: entente, entento, ententa, dentera, estente, destera\n",
      "\n",
      "8-letter spanish samples: pestente, entadona, enangare, mendadon, mentedon, lestente\n",
      "8-letter spanish top: dentente, destente, ententes, ententen, contente, ententos\n",
      "\n",
      "9-letter spanish samples: deserenes, estantara, parsperas, enoradado, derienaro, untientas\n",
      "9-letter spanish top: ententera, quentente, questente, dententes, ententero, destentes\n",
      "\n",
      "10-letter spanish samples: tacentento, ustarabosa, veranosano, estaresela, adondendel, mirestrese\n",
      "10-letter spanish top: ententente, ententento, entententa, dententera, estentente, destentera\n",
      "\n",
      "11-letter spanish samples: lasperecido, eromisconas, estentelare, estestrados, neserentamo, lestrescria\n",
      "11-letter spanish top: dententente, destentente, entententes, contentente, entententos, sententente\n",
      "\n",
      "12-letter spanish samples: prensiterres, tentedadelos, nosantentosa, rentestadosa, destaberados, belananterro\n",
      "12-letter spanish top: entententera, quententente, questentente, dentententes, entententero, quententento\n",
      "\n",
      "\n",
      "1-letter french samples: à, à, à, a, a, à\n",
      "1-letter french top: à, a, t, e, y, s\n",
      "\n",
      "2-letter french samples: de, ce, re, le, te, re\n",
      "2-letter french top: de, le, re, et, ce, se\n",
      "\n",
      "3-letter french samples: che, une, ent, des, ine, int\n",
      "3-letter french top: que, les, des, ent, une, res\n",
      "\n",
      "4-letter french samples: aute, ente, rant, mait, come, lers\n",
      "4-letter french top: lait, mait, ille, sait, fait, ques\n",
      "\n",
      "5-letter french samples: était, fante, coute, rente, êtres, quere\n",
      "5-letter french top: était, avait, illes, delle, entre, elles\n",
      "\n",
      "6-letter french samples: vantre, antait, leupas, maises, vitait, tourit\n",
      "6-letter french top: quelle, entait, delles, entent, celles, dement\n",
      "\n",
      "7-letter french samples: bierant, saitint, graitre, sondent, lentait, ettrent\n",
      "7-letter french top: quelles, lestait, destait, comment, laitait, maitait\n",
      "\n",
      "8-letter french samples: levaille, retemmet, étourine, lantelle, jettente, couteure\n",
      "8-letter french top: avaitait, questait, quellait, quellent, lestaite, destaite\n",
      "\n",
      "9-letter french samples: l'aielles, affratres, entaitene, laitintre, étaitures, houranter\n",
      "9-letter french top: dementait, ententent, laientait, maientait, dementent, entaitent\n",
      "\n",
      "10-letter french samples: pourquaite, mantencons, serrantent, destaitait, deurensont, contremait\n",
      "10-letter french top: lestaitait, destaitait, laitentait, maitentait, lestaitent, destaitent\n",
      "\n",
      "11-letter french samples: madantaient, ditemention, lestaierent, maisantrant, ditaitaites, avensientre\n",
      "11-letter french top: étaitentent, avaitentent, commentente, entententre, étaitentres, maitrentait\n",
      "\n",
      "12-letter french samples: laitandestre, avantaimeure, remaisancins, vieraittaite, deutourestre, vaitemmentre\n",
      "12-letter french top: demententait, ententaitait, dementaitait, demententent, entaitentent, entaitaitait\n",
      "\n",
      "\n",
      "1-letter english samples: a, i, a, a, a, a\n",
      "1-letter english top: a, i, e, o, s, d\n",
      "\n",
      "2-letter english samples: th, to, he, he, in, he\n",
      "2-letter english top: he, re, in, to, th, of\n",
      "\n",
      "3-letter english samples: had, and, the, and, med, hed\n",
      "3-letter english top: the, and, her, his, whe, hat\n",
      "\n",
      "4-letter english samples: ande, this, hers, dere, call, hind\n",
      "4-letter english top: ther, ithe, thed, that, then, this\n",
      "\n",
      "5-letter english samples: hishe, saing, theat, thead, there, there\n",
      "5-letter english top: there, withe, thand, hathe, nothe, ither\n",
      "\n",
      "6-letter english samples: ithate, thered, thered, anchis, hathed, theren\n",
      "6-letter english top: thathe, thered, theres, ithere, theand, wither\n",
      "\n",
      "7-letter english samples: eantere, withere, thethes, thateat, hadared, watered\n",
      "7-letter english top: theathe, therthe, thather, withere, therand, thereat\n",
      "\n",
      "8-letter english samples: heresine, stedeate, thentere, hisheres, hathenes, thathere\n",
      "8-letter english top: therethe, thathere, therithe, thereand, thereare, therathe\n",
      "\n",
      "9-letter english samples: thatheand, thimencer, indelithe, asentinge, theathere, thereente\n",
      "9-letter english top: thereathe, thathathe, thathered, theathere, therether, therthere\n",
      "\n",
      "10-letter english samples: theadangle, thathemand, hernithere, therefathe, thandeathe, untintheat\n",
      "10-letter english top: therethere, thereather, therithere, ithereathe, theathathe, thatheathe\n",
      "\n",
      "11-letter english samples: hishisecher, therearther, thatheadart, ithereather, thathersted, thereathery\n",
      "11-letter english top: thereathere, therethered, thathathere, withereathe, thereathand, therithered\n",
      "\n",
      "12-letter english samples: streanishere, thereathered, hereatherste, theatheathed, andefarithis, therenithere\n",
      "12-letter english top: thereathathe, thereathered, thereatheres, ithereathere, theathathere, thatheathere\n",
      "\n",
      "\n",
      "1-letter polish samples: a, w, o, i, u, a\n",
      "1-letter polish top: i, w, a, z, o, s\n",
      "\n",
      "2-letter polish samples: do, za, po, wy, na, na\n",
      "2-letter polish top: na, za, ni, po, do, to\n",
      "\n",
      "3-letter polish samples: oni, ose, zie, wie, cze, mne\n",
      "3-letter polish top: nie, wie, sie, mie, się, cie\n",
      "\n",
      "4-letter polish samples: siem, niał, trze, pani, obie, anie\n",
      "4-letter polish top: niem, dzie, przy, prze, anie, wiem\n",
      "\n",
      "5-letter polish samples: nieba, parzy, wodzi, spona, zawie, siwie\n",
      "5-letter polish top: panie, powie, odzie, zanie, tanie, niego\n",
      "\n",
      "6-letter polish samples: wiebie, tedzie, przach, czecie, dziany, niesza\n",
      "6-letter polish top: nienie, wienie, podzie, niecie, niemie, nierzy\n",
      "\n",
      "7-letter polish samples: taderzy, nadniem, zanicza, podziem, namiema, niedzie\n",
      "7-letter polish top: niedzie, przenie, wiedzie, nieniem, dzienie, anienie\n",
      "\n",
      "8-letter polish samples: aszeciła, proszana, przeprze, przedzie, nieniche, miaterze\n",
      "8-letter polish top: panienie, nieranie, powienie, przedzie, dziedzie, strzenie\n",
      "\n",
      "9-letter polish samples: tenieczał, onieszych, horawiata, lieczniem, ataraniem, garawiena\n",
      "9-letter polish top: nienienie, wienienie, podzienie, nierzenie, nieniecie, niecienie\n",
      "\n",
      "10-letter polish samples: mawieczała, ranierwiem, aniedzieci, postrzecie, nienieniem, nilniedzie\n",
      "10-letter polish top: nieniedzie, przenienie, wieniedzie, nienieniem, dzienienie, niedzianie\n",
      "\n",
      "11-letter polish samples: obieszczyna, wierestarza, wytnierzecz, miedziatnie, powyrzeniem, ustrzeszych\n",
      "11-letter polish top: niedziedzie, panienienie, przeniedzie, wiedziedzie, nieranienie, nienieranie\n",
      "\n",
      "12-letter polish samples: nicenierawie, dowystałamie, niedziegorze, dzierzeniego, szarażdziawy, niedziewstan\n",
      "12-letter polish top: nienienienie, wienienienie, podzienienie, nienieniecie, nieniecienie, niecienienie\n",
      "\n",
      "\n",
      "1-letter italian samples: e, a, e, a, e, a\n",
      "1-letter italian top: e, a, i, o, è, s\n",
      "\n",
      "2-letter italian samples: le, in, re, so, in, no\n",
      "2-letter italian top: di, la, re, no, se, le\n",
      "\n",
      "3-letter italian samples: ano, che, ton, pre, sat, pro\n",
      "3-letter italian top: che, the, pre, ore, con, pro\n",
      "\n",
      "4-letter italian samples: cola, rion, more, fole, coli, seno\n",
      "4-letter italian top: pare, pere, dere, alla, cone, core\n",
      "\n",
      "5-letter italian samples: derne, della, morna, calli, mormi, senta\n",
      "5-letter italian top: della, delle, nella, nelle, dello, chere\n",
      "\n",
      "6-letter italian samples: stente, inonte, forion, ledere, colina, davare\n",
      "6-letter italian top: quella, quelle, inella, inelle, antere, conone\n",
      "\n",
      "7-letter italian samples: dell'la, dissete, contale, dellete, lestano, gioneli\n",
      "7-letter italian top: conella, nonella, dellare, conelle, nonelle, dellore\n",
      "\n",
      "8-letter italian samples: antesure, nononela, sionella, ancoltre, nosoneli, sonatene\n",
      "8-letter italian top: contente, sentente, tentente, contento, gionella, pronella\n",
      "\n",
      "9-letter italian samples: dineletra, parinasti, venessere, soissiala, tesandone, lectrante\n",
      "9-letter italian top: cononella, antentere, nononella, comentere, cononelle, intentere\n",
      "\n",
      "10-letter italian samples: nonolavate, dessenbere, comantente, inostionti, mentorario, erantentin\n",
      "10-letter italian top: antentente, contentere, sententere, nontentere, dellatente, tententere\n",
      "\n",
      "11-letter italian samples: dellarestra, thelletento, entendorano, inellentere, pentialache, rentermione\n",
      "11-letter italian top: contentente, sententente, nontentente, tententente, contentento, dellandella\n",
      "\n",
      "12-letter italian samples: inaliceritio, berratorrion, trancomompre, dalitedelini, conestionare, manterovesto\n",
      "12-letter italian top: antententere, comententere, intententere, gutententere, conontentere, prententente\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "for language in trigram_languages:\n",
    "    for word_length in xrange(1, 13):\n",
    "        samples, top = language.sample_mh(word_length, max_to_store=6, n_runs=100, n_samples_per_run=1000*word_length)\n",
    "\n",
    "        samples_joined = u\", \".join(word for word, prob in samples[:6])\n",
    "        print u\"{}-letter {} samples: {}\".format(word_length, language.info.name, samples_joined)\n",
    "        \n",
    "        top_joined = \", \".join(word for prob, word in reversed(sorted(top, key=itemgetter(0))))\n",
    "        print u\"{}-letter {} top: {}\\n\".format(word_length, language.info.name, top_joined)\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encouragingly, the top-likelihood sampled words match the exhaustively-searched words well! Also encouragingly, the \"top\" words found by the sampler tend to be somewhat consistent between runs. These clues indicate that the sampler is probably not completely broken.\n",
    "\n",
    "These words look pretty good. Once they get long, though, certain three-letter sequences of letters start to repeat (e.g. \"dendendenden\" in fake-German). This makes a lot of sense, since the model is only aware of the three-letter neighborhood next to each letter.\n",
    "\n",
    "Using a sampling algorithm to search for high-likelihood points is a slight abuse of the sampler, since that's not what a sampler does. It works decently here, probably because the areas it's exploring aren't very large. To get better top-likelihood words, it would be best to use a real search algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Detecting foreign-looking words\n",
    "=================\n",
    "Time for some more fun! For this tangent, I'll go through the dictionary of each language to find words that seem to belong to other languages. Some of these are obviously loan words, but other ones (e.g. for Latin) just look foreign.\n",
    "\n",
    "In order to get a ranking, I've simply taken the ratio of the likelihood in the destination languages to the likelihood in the source language. Note that this causes longer words to be chosen, which is ranking the words by a kind of *statistical significance*. A more Bayesian approach would entail dividing this ratio by word length, which would more closely measure *effect size*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the likelihood of each word in each language\n",
    "\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "\n",
    "all_words = set(chain(*(language.dictionary for language in trigram_languages)))\n",
    "\n",
    "word_to_language_to_likelihood = defaultdict(dict)\n",
    "for word in all_words:\n",
    "    for language in trigram_languages:\n",
    "        word_with_tokens = u\" {} \".format(word)\n",
    "        word_to_language_to_likelihood[word][language.info.name] = language.get_word_log_likelihood(word_with_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latin words that look german: andrachnen, andrachlen, dendrachaten, pyrrhichen, jungebar\n",
      "latin words that look spanish: decharmidabar, decharmidabas, apolactizabar, decharmidandos, decharmidando\n",
      "latin words that look french: branchai, transfigurans, parochai, chondrillai, ravennai\n",
      "latin words that look english: asty, branchad, spathad, chelyin, gry\n",
      "latin words that look polish: chrysoprasie, onycha, sardonycha, zelotypo, chrysopastom\n",
      "latin words that look italian: leopardale, leopardali, buttutti, chalaziai, leopardi\n",
      "\n",
      "german words that look latin: simplicissimus, investitionsquote, computersimulation, quantitative, quantitativ\n",
      "german words that look spanish: casablanca, violoncello, capriccio, carpaccio, colorado\n",
      "german words that look french: journaille, croissants, croissant, quantitativ, montreux\n",
      "german words that look english: heathrow, clooney, theatersaison, everybody, tomorrow\n",
      "german words that look polish: psychosoziale, psychodrama, politologie, napoli, zbigniew\n",
      "german words that look italian: caravaggio, giacometti, bruttosozialprodukts, violoncello, cappuccino\n",
      "\n",
      "spanish words that look latin: nequáquam, cumquibus, súmmum, venimécum, tuáutem\n",
      "spanish words that look german: fahrenheit, kirsch, kitsch, angström, sandwich\n",
      "spanish words that look french: tournée, soufflé, forfait, affaire, moisés\n",
      "spanish words that look english: overbooking, ranking, washingtoniano, footing, windsurfing\n",
      "spanish words that look polish: eczema, sandwich, zinnia, alcuzcuzu, nazi\n",
      "spanish words that look italian: neonazi, chipichipi, nequáquam, katiuska, chischil\n",
      "\n",
      "french words that look latin: praesidium, praesidiums, aegopodium, caesium, cuproaluminium\n",
      "french words that look german: knickerbockers, breitschwanz, zwieback, weltanschauung, weltanschauungs\n",
      "french words that look spanish: zapateado, pronunciamiento, aficionado, smorzando, zapateados\n",
      "french words that look english: knickerbockers, sweepstake, shocking, skinhead, shopping\n",
      "french words that look polish: biodynamie, dyschromatopsie, zymotechnie, szlachta, zwieback\n",
      "french words that look italian: aggiornamento, prosciutto, taleggio, grazioso, benzodiazepine\n",
      "\n",
      "english words that look latin: praesidium, gynaecium, pseudopodium, nonequilibrium, anaerobium\n",
      "english words that look german: weltschmerz, zeitgeber, braunschweiger, weltschmerzes, lebensraum\n",
      "english words that look spanish: haciendados, osteosarcomata, hacendados, abracadabra, piasaba\n",
      "english words that look french: etiquettes, jeux, jambeaux, oeuvres, questionnaires\n",
      "english words that look polish: psychodynamic, psychodrama, psychobiologic, hypoglycemia, psychodynamics\n",
      "english words that look italian: razzamatazz, razzamatazzes, osteosarcomata, pharmacopoeia, prosopopoeia\n",
      "\n",
      "polish words that look latin: plusquamperfectum, plusquamperfecta, plusquamperfectami, plusquamperfectom, experimentum\n",
      "polish words that look german: verheugen, langenscheidt, verheugenem, eisenstein, verheugenom\n",
      "polish words that look spanish: barranquillo, quandoque, divertimento, seguidillo, superinteligencjo\n",
      "polish words that look french: quantité, l'improductivité, désintéressement, divertissement, cloisonné\n",
      "polish words that look english: overclocking, overbooking, overlock, verhoeven, knorringit\n",
      "polish words that look italian: l'improductivité, ultrasonokardiografio, divertissement, divertimento, verrocchio\n",
      "\n",
      "italian words that look latin: cumquibus, protoquamquam, omnium, quamquam, omnibus\n",
      "italian words that look german: kinderheim, wehrmacht, fahrenheit, breitschwanz, zeitgeist\n",
      "italian words that look spanish: calvados, blablabla, endecasillaba, abracadabra, descamisado\n",
      "italian words that look french: nouveau, connaisseur, tourniquet, jeans, voyeur\n",
      "italian words that look english: whiskey, sherry, mystery, hickory, workshop\n",
      "italian words that look polish: mystery, niego, milady, browniana, pony\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from fake_words.fake_words import try_to_store\n",
    "\n",
    "N_TO_STORE = 5\n",
    "\n",
    "# Loop over every pair of languages\n",
    "for language_source in trigram_languages:\n",
    "    for language_dest in trigram_languages:\n",
    "        # A heap to store our best results\n",
    "        best_words = []\n",
    "\n",
    "        if language_dest is language_source:\n",
    "            continue\n",
    "    \n",
    "        for word in language_source.dictionary:\n",
    "            \n",
    "            # Subtraction produces a ratio because these are log probabilities\n",
    "            ratio = (word_to_language_to_likelihood[word][language_dest.info.name] - \n",
    "                     word_to_language_to_likelihood[word][language_source.info.name])\n",
    "            try_to_store(best_words, word, ratio, N_TO_STORE)\n",
    "\n",
    "        best_words_pretty = \", \".join(word for prob, word in reversed(sorted(best_words, key=itemgetter(0))))\n",
    "        print u\"{} words that look {}: {}\".format(language_source.info.name, language_dest.info.name, best_words_pretty)\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All right, now I'm having fun!\n",
    "\n",
    "I always feel slightly offended when I see the words that look English. \"Whiskey.\" \"Sweepstake.\" \"Branchad.\" \"Spathad.\" \"Overbooking!\" Is this what English looks like to non-English-speakers? To make myself feel better, I imagine a heavy-set German man with a disproportionately large mustache saying, \"braunschweiger,\" \"breitschwanz,\" and \"weltschmerzes\" very seriously.\n",
    "\n",
    "I also enjoy that the algorithm classifies \"jeans\" as French. Which is true, just ask any pair of Jeans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detecting distinctive words\n",
    "===============\n",
    "A similar application of this model is to detect distinctive-looking words, i.e. words that are likely in their source language but very unlikely in all other languages.\n",
    "\n",
    "To model this, I'm ranking each word with the ratio between its likelihood in its source language to its likelihood in *any* other language. As above, since I'm not dividing by word length, this tends to select longer words.\n",
    "\n",
    "Note that these distinctive-looking words are inherently relative to the other languages modeled. Adding another Germanic language, for example, might change the words that were reported as most distinctively German."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most latin-looking words: praeconsumpturorum, circumcumulaturorum, circummoeniturorum, circumequitaturorum, praeconsumpturum, praeconsumpturarum, curculiunculorum, praeconsumptorum, circumcumulaturum, circumcumulaturarum\n",
      "\n",
      "most german-looking words: geschwindigkeitsbeschränkungen, weihnachtsgeschenken, verschleißerscheinungen, weihnachtsgeschichten, gleichgeschlechtlichen, zusammengehörigkeitsgefühl, sicherheitseinrichtungen, selbstverständlichkeiten, wahrscheinlichkeiten, geschwindigkeitsbeschränkung\n",
      "\n",
      "most spanish-looking words: jerarquización, españolización, encuadernación, vulgarización, despoblación, despolarización, engrasación, señalización, despresurización, especialización\n",
      "\n",
      "most french-looking words: bienfaiteurs, broussailleux, jouvenceaux, poursuivraient, bienfaiteur, souviendraient, malheureux, entrouvriraient, entrouvrirait, vouvoyaient\n",
      "\n",
      "most english-looking words: knowledgeably, acknowledgedly, crookedly, thoughtlessly, heartbreakingly, greatheartedly, thoughtfully, overpoweringly, shortsightedly, stoutheartedly\n",
      "\n",
      "most polish-looking words: niedziewięćdziesięciogodzinny, niedziewięćdziesięciogodzinnych, niedziewięćdziesięciogodzinnym, niedziewięćdziesięciogodzinną, nieosiemdziesięciodziewięcioletniemu, niedziewięciotysięczny, niedziewięciotysięcznych, nieosiemdziesięciodziewięcioletniego, niepozanieczyszczany, niedziewięćdziesięciogodzinnymi\n",
      "\n",
      "most italian-looking words: riaggeggiavano, leopardeggiava, taglieggiavano, riaggeggiavamo, sfotticchiavano, avvantaggiavano, riaggeggiava, picchettaggio, assottigliavano, dottoreggiavano\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N_TO_STORE = 10\n",
    "\n",
    "for language_source in trigram_languages:\n",
    "    # A heap to store our best results\n",
    "    best_words = []\n",
    "\n",
    "    for word in language_source.dictionary:\n",
    "        likelihood_source = word_to_language_to_likelihood[word][language_source.info.name]\n",
    "        \n",
    "        ratios = []\n",
    "        for language_dest in trigram_languages:\n",
    "            if language_dest is language_source:\n",
    "                continue\n",
    "\n",
    "            # Subtraction produces a ratio because these are log probabilities\n",
    "            ratios.append(likelihood_source - word_to_language_to_likelihood[word][language_dest.info.name])\n",
    "        \n",
    "        try_to_store(best_words, word, min(ratios), N_TO_STORE)\n",
    "                     \n",
    "    best_words_pretty = \", \".join(word for prob, word in reversed(sorted(best_words, key=itemgetter(0))))\n",
    "    print u\"most {}-looking words: {}\\n\".format(language_source.info.name, best_words_pretty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Geschwindigkeitsbeschränkung,\" WOOOO! That means \"speed limit.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Conclusion\n",
    "=====\n",
    "Hopefully this article has illustrated the strengths and limitations of using a Markov model for word structure. It seems to work nicely for short words, but for long words it might need to be augmented by 4-grams or 5-grams, or it might need to be replaced by a more complex model.\n",
    "\n",
    "This is a good start, but this is just the tip of the iceberg in terms of what could be done with a word structure model. Here are a couple ideas:\n",
    "* Easily and automatically generate plausible wrong answers for multiple-choice spelling tests. This could be useful for language learning.\n",
    "* Train the model on a fake language, and then sample words to create new words in that language.\n",
    "* Generate a name for your startup (I believe that [Wordoid](http://wordoid.com/) may already be using a Markov model!).\n",
    "\n",
    "There are also many NLP problems that could make use of word structure modeling: optical character recognition, speech recognition, swipe typing, spell checking, web search, and machine translation come to mind. I haven't found any examples of word structure modeling, perhaps because people communicate primarily using a vocabulary that can be easily stored in a dictionary. A word structure model could be useful to help to keep up with changing vocabularies, and to handle things such as names, which follow common structural patterns but are too numerous to fit into a dictionary.\n",
    "\n",
    "Additional Reading\n",
    "-----------------\n",
    "I didn't do much research on the topic before writing this, so I'm probably ignorant of a lot of great work out there. If you know of something that does word structure modeling, please let me know!\n",
    "\n",
    "* A [super fun paper](http://nl.ijs.si/janes/wp-content/uploads/2014/09/choudhuryothers07.pdf) from the Malaviya National Institute of Technology in Jaipur that uses a letter-level HMM to translate SMS abbreviations into their standard English equivalents.\n",
    "* Peter Norvig wrote a [classic blog post](http://norvig.com/spell-correct.html) on how to write an extremely simple spelling correction algorithm.\n",
    "* [Here](http://www.aclweb.org/anthology/N03-1018) is a paper on word structure modeling for OCR applications, by Kolak, Byrne, and Resnik.\n",
    "* Andrej Karpathy wrote a [blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) about using recurrent neural networks to do character-level language modeling.\n",
    "\n",
    "Thanks\n",
    "------\n",
    "Thanks to [Eric Kernfeld](http://erickernfeld.yolasite.com/), [Victor Jakubiuk](http://jakubiuk.net), [Brandon Reiss](http://www.brandonreiss.com/), and Max Livingston for suggesting many improvements to this!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
